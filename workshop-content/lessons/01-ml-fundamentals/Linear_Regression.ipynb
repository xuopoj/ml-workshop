{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **线性回归Python底层实现**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 案例内容介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性回归是机器学习中最基本的模型，用来拟合自变量和因变量之间呈现线性关系的数据，当自变量只有一个时我们称使用的回归模型是一元线性回归模型，当自变量有多个时称使用的回归模型是多元线性回归模型。根据已知数据，求解线性回归模型的参数最常用到的方法是最小二乘法，求解使得损失函数取得最小值的模型参数的解析解或者使用梯度下降算法求得最优的模型参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验目标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本实验通过Python底层代码，实现对最优线性回归模型参数的解析解的求解过程。帮助大家加深线性回归模型的基本求解原理。掌握了基本原理的代码实现后，通过一个简单的工具包调用过程帮助大家掌握快速实现线性回归模型的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验步骤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**数据集说明：**   本实验使用的是构造的数据集，数据构造的过程在代码中有明确显示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、一元线性回归Python底层实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T01:53:20.625155800Z",
     "start_time": "2026-01-15T01:53:19.218148700Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 一元线性回归的实现\n",
    "\n",
    "#导入matplotlib库，主要用于可视化\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#引入本地字体文件，否则中文会有乱码 \n",
    "#font_set = FontProperties(fname=r\"./work/ simsun.ttc\", size=12)\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import numpy as np\n",
    "\n",
    "# 构造用于训练的数据集\n",
    "x_train = [4,8,5,10,12]\n",
    "y_train = [20,50,30,70,60]\n",
    "\n",
    "# 画图函数\n",
    "def draw(x_train,y_train):\n",
    "    plt.scatter(x_train, y_train)\n",
    "\n",
    "# 定义函数求得斜率w和截距b\n",
    "# 使用最小二乘法对斜率和截距求导并使得导数值等于0求解出斜率和截距\n",
    "def fit(x_train,y_train): \n",
    "    size = len(x_train)\n",
    "    numerator = 0 #初始化分子\n",
    "    denominator = 0#初始化分母\n",
    "    for i in range(size):\n",
    "        numerator += (x_train[i]-np.mean(x_train))*(y_train[i]-np.mean(y_train))  \n",
    "        denominator += (x_train[i]-np.mean(x_train))**2\n",
    "    w = numerator/denominator\n",
    "    b = np.mean(y_train)-w*np.mean(x_train)\n",
    "    return w,b\n",
    "\n",
    "#根据斜率w和截距b，输入x计算输出值\n",
    "def predict(x,w,b):  \n",
    "    #预测模型\n",
    "    y = w*x+b\n",
    "    return y\n",
    "\n",
    "# 根据W,B画图\n",
    "def fit_line(w,b):\n",
    "#测试集进行测试，并作图\n",
    "    x = np.linspace(4,15,9)  #linspace 创建等差数列的函数    #numpy.limspace(start,stop,num,endpoint=True,retstep=False,dtype=None,axis=0#) \n",
    "    y = w*x+b\n",
    "    plt.plot(x,y)\n",
    "    plt.show()\n",
    "    \n",
    "if __name__ ==\"__main__\":\n",
    "    draw(x_train,y_train)\n",
    "    w,b = fit(x_train,y_train)\n",
    "    print(w,b) #输出斜率和截距\n",
    "fit_line(w,b) #绘制预测函数图像"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、多元线性回归的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T01:56:33.784741300Z",
     "start_time": "2026-01-15T01:56:33.745510800Z"
    }
   },
   "outputs": [],
   "source": [
    "# 多元线性回归的实现\n",
    "# 导入模块\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 构造数据，前三列表示自变量X，最后一列表示因变量Y\n",
    "data = np.array([[3,2,9,20],\n",
    "                  [4,10,2,72],\n",
    "                  [3,4,9,21],\n",
    "                  [12,3,4,20]])\n",
    "print(\"data:\",data,\"\\n\")\n",
    "\n",
    "X=data[:,:-1]\n",
    "Y=data[:,-1]\n",
    "\n",
    "X=np.asmatrix(np.c_[np.ones(X.shape[0]),X])# 为系数矩阵增加常数项系数，按行连接两个矩阵\n",
    "Y=np.asmatrix(Y)# 数组转化为矩阵\n",
    "\n",
    "print(\"X:\",X,\"\\n\")\n",
    "print(\"Y:\",Y,\"\\n\")\n",
    "\n",
    "# 根据最小二乘法的目标函数求导为0得到最优参数向量B的解析解公式如下，可以直接求取最优参数向量\n",
    "B=np.linalg.inv(X.T*X)*(X.T)*(Y.T) \n",
    "print(\"B:\",B,\"\\n\")# 输出系数,第一项为常数项，其他为回归系数\n",
    "print(\"1,60,60,60预测结果：\",np.asmatrix([1,60,60,60])*B,\"\\n\")#预测结果\n",
    "\n",
    "\n",
    "# 相关系数\n",
    "Q_e=0\n",
    "Q_E=0\n",
    "Y_mean=np.mean(Y)\n",
    "for i in range(Y.size):\n",
    "    Q_e+=pow(np.array((Y.T)[i]-X[i]*B),2)\n",
    "    Q_E+=pow(np.array(X[i]*B)-Y_mean,2)\n",
    "R2=Q_E/(Q_e+Q_E)\n",
    "print(\"R2\",R2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、线性回归第三方库实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T02:00:10.965383100Z",
     "start_time": "2026-01-15T02:00:10.918020300Z"
    }
   },
   "outputs": [],
   "source": [
    "# 导入sklearn下的LinearRegression 方法\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "model = LinearRegression()\n",
    "\n",
    "# 构造用于训练的数据集\n",
    "x_train = np.array([[2,4],[5,8],[5,9],[7,10],[9,12]])\n",
    "y_train = np.array([20,50,30,70,60])\n",
    "\n",
    "# 训练模型并输出模型系数和训练结果\n",
    "model.fit(x_train,y_train)\n",
    "#fit(x,y,sample_weight=None)x:训练集 y:目标值 sample_weight:每个样本的个数\n",
    "#coef_ 系数w,intercept_截距\n",
    "print(model.coef_) #输出系数w\n",
    "print(model.intercept_) #输出截距b\n",
    "print(model.score(x_train,y_train)) #输出模型的评估分数R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "本实验通过Python底层代码，实现了对最优线性回归模型参数的解析解的求解过程。在实际应用中，基于对算法的基本原理的了解，为了代码的简洁性和高效率，通常是直接调用已有的算法包。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
