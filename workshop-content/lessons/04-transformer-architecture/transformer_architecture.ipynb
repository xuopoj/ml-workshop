{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬ä¸‰è¯¾ï¼šTransformeræ¶æ„\n",
    "\n",
    "**è¯¾ç¨‹æ—¶é•¿**: 1å°æ—¶  \n",
    "**è¯¾ç¨‹ç›®æ ‡**: \n",
    "- ç†è§£Transformerçš„æ ¸å¿ƒç»„ä»¶\n",
    "- æŒæ¡æ³¨æ„åŠ›æœºåˆ¶çš„åŸç†\n",
    "- ä¸ºå­¦ä¹ å¤§è¯­è¨€æ¨¡å‹æ‰“ä¸‹åŸºç¡€\n",
    "\n",
    "**å‚è€ƒè®ºæ–‡**: [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (2017)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç›®å½•\n",
    "\n",
    "1. Transformerçš„è¯ç”ŸèƒŒæ™¯\n",
    "2. æ•´ä½“æ¶æ„æ¦‚è§ˆ\n",
    "3. æ ¸å¿ƒç»„ä»¶è¯¦è§£\n",
    "   - 3.1 Embedding (è¯åµŒå…¥)\n",
    "   - 3.2 Positional Encoding (ä½ç½®ç¼–ç )\n",
    "   - 3.3 Self-Attention (è‡ªæ³¨æ„åŠ›)\n",
    "   - 3.4 Multi-Head Attention (å¤šå¤´æ³¨æ„åŠ›)\n",
    "   - 3.5 Feed-Forward Network (å‰é¦ˆç½‘ç»œ)\n",
    "   - 3.6 Residual Connection (æ®‹å·®è¿æ¥)\n",
    "   - 3.7 Layer Normalization (å±‚å½’ä¸€åŒ–)\n",
    "   - 3.8 Masking (æ©ç )\n",
    "4. Mixture of Experts (MoE)\n",
    "5. Transformerçš„ä¼˜åŠ¿ä¸å±€é™\n",
    "6. æ€»ç»“ä¸å±•æœ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import seaborn as sns\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º\n",
    "rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# è®¾ç½®ç»˜å›¾é£æ ¼\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Transformerçš„è¯ç”ŸèƒŒæ™¯\n",
    "\n",
    "### 1.1 åºåˆ—å¤„ç†çš„å†å²\n",
    "\n",
    "åœ¨Transformerå‡ºç°ä¹‹å‰ï¼Œå¤„ç†åºåˆ—æ•°æ®ï¼ˆå¦‚æ–‡æœ¬ã€æ—¶é—´åºåˆ—ï¼‰ä¸»è¦ä¾é :\n",
    "\n",
    "#### RNNå®¶æ— (1980s-2010s)\n",
    "```\n",
    "RNN â†’ LSTM â†’ GRU\n",
    "```\n",
    "\n",
    "**ä¼˜ç‚¹**:\n",
    "- å¤©ç„¶å¤„ç†åºåˆ—æ•°æ®\n",
    "- ç†è®ºä¸Šå¯ä»¥æ•æ‰é•¿è·ç¦»ä¾èµ–\n",
    "\n",
    "**ç¼ºç‚¹**:\n",
    "- âŒ **é¡ºåºå¤„ç†**: æ— æ³•å¹¶è¡ŒåŒ–ï¼Œè®­ç»ƒæ…¢\n",
    "- âŒ **æ¢¯åº¦æ¶ˆå¤±**: éš¾ä»¥æ•æ‰é•¿è·ç¦»ä¾èµ–\n",
    "- âŒ **è®°å¿†ç“¶é¢ˆ**: æ‰€æœ‰ä¿¡æ¯å‹ç¼©åˆ°å›ºå®šå¤§å°çš„éšçŠ¶æ€\n",
    "\n",
    "### 1.2 Attentionçš„å¼•å…¥ (2015)\n",
    "\n",
    "**Bahdanau Attention** (ç”¨äºæœºå™¨ç¿»è¯‘):\n",
    "- å…è®¸è§£ç å™¨å…³æ³¨è¾“å…¥åºåˆ—çš„ä¸åŒä½ç½®\n",
    "- è§£å†³äº†RNNçš„ä¿¡æ¯ç“¶é¢ˆé—®é¢˜\n",
    "- ä½†ä»ç„¶ä¾èµ–RNNä½œä¸ºéª¨å¹²\n",
    "\n",
    "### 1.3 Transformerçš„é©å‘½ (2017)\n",
    "\n",
    "**æ ¸å¿ƒæ€æƒ³**: \"Attention Is All You Need\"\n",
    "- **å®Œå…¨æŠ›å¼ƒRNN/CNN**\n",
    "- **çº¯ç²¹åŸºäºæ³¨æ„åŠ›æœºåˆ¶**\n",
    "- **å¹¶è¡ŒåŒ–è®­ç»ƒ**\n",
    "- **æ›´å¥½åœ°æ•æ‰é•¿è·ç¦»ä¾èµ–**\n",
    "\n",
    "### 1.4 Transformerçš„å½±å“\n",
    "\n",
    "```\n",
    "Transformer (2017)\n",
    "    â†“\n",
    "â”œâ”€ NLP: BERT (2018), GPTç³»åˆ—, T5, ...\n",
    "â”œâ”€ Vision: ViT (2020), DETR, ...\n",
    "â”œâ”€ Speech: Whisper, ...\n",
    "â””â”€ Multi-modal: CLIP, Flamingo, GPT-4, ...\n",
    "```\n",
    "\n",
    "Transformerå·²æˆä¸º**ç°ä»£æ·±åº¦å­¦ä¹ çš„ç»Ÿä¸€æ¶æ„**ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. æ•´ä½“æ¶æ„æ¦‚è§ˆ\n",
    "\n",
    "åŸå§‹Transformeræ˜¯ä¸€ä¸ª**ç¼–ç å™¨-è§£ç å™¨**æ¶æ„ï¼ˆç”¨äºæœºå™¨ç¿»è¯‘ï¼‰:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                 Transformer                     â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚    Encoder (ç¼–ç å™¨) â”‚    Decoder (è§£ç å™¨)        â”‚\n",
    "â”‚                    â”‚                            â”‚\n",
    "â”‚  [Nxç¼–ç å™¨å±‚]       â”‚   [Nxè§£ç å™¨å±‚]             â”‚\n",
    "â”‚    â†“               â”‚      â†“                     â”‚\n",
    "â”‚  Multi-Head Attn   â”‚   Masked Multi-Head Attn   â”‚\n",
    "â”‚    â†“               â”‚      â†“                     â”‚\n",
    "â”‚  Feed Forward      â”‚   Cross Attention          â”‚\n",
    "â”‚    â†“               â”‚      â†“                     â”‚\n",
    "â”‚  (æ¯å±‚æœ‰æ®‹å·®+å½’ä¸€åŒ–) â”‚   Feed Forward             â”‚\n",
    "â”‚                    â”‚      â†“                     â”‚\n",
    "â”‚                    â”‚   (æ¯å±‚æœ‰æ®‹å·®+å½’ä¸€åŒ–)        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        â†‘                          â†“\n",
    "    è¾“å…¥åºåˆ—                    è¾“å‡ºåºåˆ—\n",
    "```\n",
    "\n",
    "### 2.2 ç°ä»£å˜ä½“\n",
    "\n",
    "| æ¨¡å‹ç±»å‹ | æ¶æ„ | ä»£è¡¨æ¨¡å‹ | åº”ç”¨åœºæ™¯ |\n",
    "|---------|------|---------|----------|\n",
    "| **Encoder-only** | ä»…ä½¿ç”¨ç¼–ç å™¨ | BERT, RoBERTa | æ–‡æœ¬ç†è§£ã€åˆ†ç±» |\n",
    "| **Decoder-only** | ä»…ä½¿ç”¨è§£ç å™¨ | GPTç³»åˆ—, LLaMA, Qwen | æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ |\n",
    "| **Encoder-Decoder** | å®Œæ•´æ¶æ„ | T5, BART | ç¿»è¯‘ã€æ‘˜è¦ |\n",
    "\n",
    "**å½“å‰è¶‹åŠ¿**: Decoder-onlyæ¶æ„ï¼ˆGPTèŒƒå¼ï¼‰å æ®ä¸»å¯¼åœ°ä½ï¼Œå› ä¸º:\n",
    "- æ›´ç®€æ´\n",
    "- æ›´æ˜“æ‰©å±•\n",
    "- æ€§èƒ½æ›´å¼ºï¼ˆåœ¨è¶³å¤Ÿæ•°æ®ä¸‹ï¼‰\n",
    "\n",
    "### 2.3 ä¸€ä¸ªç¼–ç å™¨å±‚çš„è¯¦ç»†ç»“æ„\n",
    "\n",
    "```\n",
    "è¾“å…¥ (X)\n",
    "  â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Multi-Head Attention   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "  â†“\n",
    "Add & Norm (æ®‹å·® + å±‚å½’ä¸€åŒ–)\n",
    "  â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Feed Forward Network  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "  â†“\n",
    "Add & Norm (æ®‹å·® + å±‚å½’ä¸€åŒ–)\n",
    "  â†“\n",
    "è¾“å‡º (ä¼ é€’ç»™ä¸‹ä¸€å±‚)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. æ ¸å¿ƒç»„ä»¶è¯¦è§£\n",
    "\n",
    "### 3.1 Embedding (è¯åµŒå…¥)\n",
    "\n",
    "#### ä»€ä¹ˆæ˜¯Embeddingï¼Ÿ\n",
    "\n",
    "å°†**ç¦»æ•£çš„è¯**æ˜ å°„åˆ°**è¿ç»­çš„å‘é‡ç©ºé—´**ã€‚\n",
    "\n",
    "```\n",
    "\"ä½ å¥½\" â†’ [0.2, -0.5, 0.8, ..., 0.3]  (dç»´å‘é‡)\n",
    "\"ä¸–ç•Œ\" â†’ [0.1, 0.7, -0.2, ..., 0.5]\n",
    "```\n",
    "\n",
    "#### ä¸ºä»€ä¹ˆéœ€è¦Embeddingï¼Ÿ\n",
    "\n",
    "1. **è¯­ä¹‰ç›¸ä¼¼çš„è¯åœ¨å‘é‡ç©ºé—´ä¸­ä¹Ÿæ¥è¿‘**\n",
    "   - \"å›½ç‹\" - \"ç”·äºº\" + \"å¥³äºº\" â‰ˆ \"å¥³ç‹\"\n",
    "   \n",
    "2. **ç¥ç»ç½‘ç»œéœ€è¦æ•°å€¼è¾“å…¥**\n",
    "   - ä¸èƒ½ç›´æ¥å¤„ç†æ–‡æœ¬\n",
    "\n",
    "3. **é™ç»´**\n",
    "   - è¯æ±‡è¡¨å¤§å°: 50,000+\n",
    "   - åµŒå…¥ç»´åº¦: 512-1024\n",
    "\n",
    "#### Embeddingçš„å®ç°\n",
    "\n",
    "æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ª**æŸ¥æ‰¾è¡¨ (Lookup Table)**:\n",
    "\n",
    "$$\n",
    "E \\in \\mathbb{R}^{V \\times d}\n",
    "$$\n",
    "\n",
    "å…¶ä¸­:\n",
    "- $V$ æ˜¯è¯æ±‡è¡¨å¤§å°\n",
    "- $d$ æ˜¯åµŒå…¥ç»´åº¦\n",
    "- æ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªè¯çš„åµŒå…¥å‘é‡\n",
    "\n",
    "#### è®­ç»ƒæ–¹å¼\n",
    "\n",
    "1. **éšæœºåˆå§‹åŒ–** + ç«¯åˆ°ç«¯è®­ç»ƒ\n",
    "2. **é¢„è®­ç»ƒè¯å‘é‡**: Word2Vec, GloVe, FastText\n",
    "3. **ä¸Šä¸‹æ–‡ç›¸å…³**: ç°ä»£LLMä¸­æ¯ä¸ªè¯çš„è¡¨ç¤ºå–å†³äºä¸Šä¸‹æ–‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç®€å•çš„Embeddingç¤ºä¾‹ï¼ˆæ¦‚å¿µæ¼”ç¤ºï¼‰\n",
    "vocab_size = 10000  # è¯æ±‡è¡¨å¤§å°\n",
    "embedding_dim = 512  # åµŒå…¥ç»´åº¦\n",
    "\n",
    "# åˆ›å»ºembeddingçŸ©é˜µï¼ˆéšæœºåˆå§‹åŒ–ï¼‰\n",
    "embedding_matrix = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
    "\n",
    "# è¯ç´¢å¼•\n",
    "word_ids = [123, 456, 789]  # \"æˆ‘ çˆ± ä¸­å›½\"\n",
    "\n",
    "# æŸ¥æ‰¾å¯¹åº”çš„è¯å‘é‡\n",
    "word_embeddings = embedding_matrix[word_ids]\n",
    "\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n",
    "print(f\"Word sequence: {word_ids}\")\n",
    "print(f\"Word embeddings shape: {word_embeddings.shape}\")\n",
    "print(f\"\\nFirst word embedding (first 10 dimensions):\\n{word_embeddings[0][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Positional Encoding (ä½ç½®ç¼–ç )\n",
    "\n",
    "#### é—®é¢˜ï¼šTransformeræ²¡æœ‰åºåˆ—æ¦‚å¿µ\n",
    "\n",
    "ä¸RNNä¸åŒï¼ŒTransformerçš„Self-Attentionæ˜¯**æ’åˆ—ä¸å˜çš„** (permutation invariant):\n",
    "\n",
    "```\n",
    "Attention(\"æˆ‘ çˆ± ä½ \") = Attention(\"ä½  çˆ± æˆ‘\") = Attention(\"çˆ± ä½  æˆ‘\")\n",
    "```\n",
    "\n",
    "è¿™æ˜¾ç„¶ä¸å¯¹ï¼æˆ‘ä»¬éœ€è¦**æ³¨å…¥ä½ç½®ä¿¡æ¯**ã€‚\n",
    "\n",
    "#### è§£å†³æ–¹æ¡ˆï¼šä½ç½®ç¼–ç \n",
    "\n",
    "å°†ä½ç½®ä¿¡æ¯ç¼–ç æˆå‘é‡,åŠ åˆ°è¯åµŒå…¥ä¸Š:\n",
    "\n",
    "$$\n",
    "\\text{Input} = \\text{Embedding} + \\text{Positional Encoding}\n",
    "$$\n",
    "\n",
    "#### æ­£å¼¦ä½ç½®ç¼–ç ï¼ˆåŸå§‹Transformerï¼‰\n",
    "\n",
    "å¯¹äºä½ç½® $pos$ å’Œç»´åº¦ $i$:\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d}}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d}}\\right)\n",
    "$$\n",
    "\n",
    "#### ä¸ºä»€ä¹ˆä½¿ç”¨æ­£å¼¦å‡½æ•°ï¼Ÿ\n",
    "\n",
    "1. **ç¡®å®šæ€§**: æ— éœ€å­¦ä¹ å‚æ•°\n",
    "2. **å¤–æ¨æ€§**: å¯ä»¥å¤„ç†è®­ç»ƒæ—¶æœªè§è¿‡çš„é•¿åº¦\n",
    "3. **ç›¸å¯¹ä½ç½®**: $PE_{pos+k}$ å¯ä»¥è¡¨ç¤ºä¸º $PE_{pos}$ çš„çº¿æ€§å‡½æ•°\n",
    "4. **ä¸åŒé¢‘ç‡**: ä¸åŒç»´åº¦æœ‰ä¸åŒçš„æ³¢é•¿ï¼Œç¼–ç ä¸åŒç²’åº¦çš„ä½ç½®ä¿¡æ¯\n",
    "\n",
    "#### ç°ä»£å˜ä½“\n",
    "\n",
    "- **Learnable Position Embedding**: BERTä½¿ç”¨å¯å­¦ä¹ çš„ä½ç½®åµŒå…¥\n",
    "- **Relative Positional Encoding**: Transformer-XL, T5\n",
    "- **Rotary Position Embedding (RoPE)**: LLaMA, Qwenä½¿ç”¨\n",
    "- **ALiBi**: åœ¨attention scoreä¸­ç›´æ¥åŠ bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ­£å¼¦ä½ç½®ç¼–ç çš„å®ç°ä¸å¯è§†åŒ–\n",
    "def positional_encoding(max_len, d_model):\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆæ­£å¼¦ä½ç½®ç¼–ç \n",
    "    \n",
    "    å‚æ•°:\n",
    "    - max_len: æœ€å¤§åºåˆ—é•¿åº¦\n",
    "    - d_model: åµŒå…¥ç»´åº¦\n",
    "    \"\"\"\n",
    "    pe = np.zeros((max_len, d_model))\n",
    "    position = np.arange(0, max_len)[:, np.newaxis]\n",
    "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "    \n",
    "    pe[:, 0::2] = np.sin(position * div_term)\n",
    "    pe[:, 1::2] = np.cos(position * div_term)\n",
    "    \n",
    "    return pe\n",
    "\n",
    "# ç”Ÿæˆä½ç½®ç¼–ç \n",
    "max_len = 100\n",
    "d_model = 512\n",
    "pe = positional_encoding(max_len, d_model)\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# çƒ­åŠ›å›¾ï¼šä½ç½®ç¼–ç çŸ©é˜µ\n",
    "im = ax1.imshow(pe, cmap='RdBu', aspect='auto')\n",
    "ax1.set_xlabel('Dimension')\n",
    "ax1.set_ylabel('Position')\n",
    "ax1.set_title('Positional Encoding Heatmap')\n",
    "plt.colorbar(im, ax=ax1)\n",
    "\n",
    "# æ›²çº¿å›¾ï¼šä¸åŒä½ç½®åœ¨å‰å‡ ä¸ªç»´åº¦çš„ç¼–ç å€¼\n",
    "for i in range(0, 20, 4):  # Display dimensions 0, 4, 8, 12, 16\n",
    "    ax2.plot(pe[:, i], label=f'Dim {i}')\n",
    "ax2.set_xlabel('Position')\n",
    "ax2.set_ylabel('Encoding Value')\n",
    "ax2.set_title('Positional Encoding Curves for Different Dimensions')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Positional encoding shape: {pe.shape}\")\n",
    "print(f\"\\nPosition 0 encoding (first 10 dimensions):\\n{pe[0, :10]}\")\n",
    "print(f\"\\nPosition 50 encoding (first 10 dimensions):\\n{pe[50, :10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Self-Attention (è‡ªæ³¨æ„åŠ›)\n",
    "\n",
    "Self-Attentionæ˜¯Transformerçš„**æ ¸å¿ƒåˆ›æ–°**ã€‚\n",
    "\n",
    "#### æ ¸å¿ƒæ€æƒ³\n",
    "\n",
    "å¯¹äºåºåˆ—ä¸­çš„æ¯ä¸ªè¯ï¼Œè®¡ç®—å®ƒä¸åºåˆ—ä¸­**æ‰€æœ‰è¯ï¼ˆåŒ…æ‹¬è‡ªå·±ï¼‰**çš„å…³è”åº¦ï¼Œç„¶ååŠ æƒæ±‚å’Œã€‚\n",
    "\n",
    "```\n",
    "ä¾‹å¥: \"The animal didn't cross the street because it was too tired.\"\n",
    "\n",
    "å½“å¤„ç†\"it\"æ—¶ï¼ŒSelf-Attentionä¼š:\n",
    "- é«˜åº¦å…³æ³¨\"animal\" âœ…\n",
    "- è¾ƒå°‘å…³æ³¨\"street\"\n",
    "- å‡ ä¹ä¸å…³æ³¨\"the\"\n",
    "```\n",
    "\n",
    "#### Query, Key, Value (Q, K, V)\n",
    "\n",
    "è¿™æ˜¯Attentionçš„ä¸‰ä¸ªæ ¸å¿ƒæ¦‚å¿µ,ç±»æ¯”æœç´¢å¼•æ“:\n",
    "\n",
    "- **Query (æŸ¥è¯¢)**: æˆ‘åœ¨æ‰¾ä»€ä¹ˆï¼Ÿ\n",
    "- **Key (é”®)**: æˆ‘æ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "- **Value (å€¼)**: æˆ‘çš„å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "\n",
    "å¯¹äºè¾“å…¥ $X \\in \\mathbb{R}^{n \\times d}$:\n",
    "\n",
    "$$\n",
    "Q = XW^Q, \\quad K = XW^K, \\quad V = XW^V\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ $W^Q, W^K, W^V \\in \\mathbb{R}^{d \\times d_k}$ æ˜¯å¯å­¦ä¹ çš„å‚æ•°ã€‚\n",
    "\n",
    "#### Scaled Dot-Product Attention\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "**è®¡ç®—æ­¥éª¤**:\n",
    "\n",
    "1. **è®¡ç®—ç›¸ä¼¼åº¦**: $QK^T \\in \\mathbb{R}^{n \\times n}$ ï¼ˆç‚¹ç§¯ï¼‰\n",
    "2. **ç¼©æ”¾**: é™¤ä»¥ $\\sqrt{d_k}$ ï¼ˆé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±ï¼‰\n",
    "3. **å½’ä¸€åŒ–**: Softmaxï¼ˆå¾—åˆ°æ³¨æ„åŠ›æƒé‡ï¼‰\n",
    "4. **åŠ æƒæ±‚å’Œ**: ä¹˜ä»¥ $V$\n",
    "\n",
    "#### ä¸ºä»€ä¹ˆè¦ç¼©æ”¾ï¼Ÿ\n",
    "\n",
    "å½“ $d_k$ å¾ˆå¤§æ—¶ï¼Œç‚¹ç§¯çš„æ–¹å·®ä¹Ÿå¾ˆå¤§ï¼Œå¯¼è‡´softmaxè¿›å…¥é¥±å’ŒåŒºï¼Œæ¢¯åº¦å¾ˆå°ã€‚\n",
    "\n",
    "$$\n",
    "\\text{Var}(QK^T) = d_k \\quad \\Rightarrow \\quad \\text{Var}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-Attentionçš„ç®€åŒ–å®ç°ï¼ˆæ¦‚å¿µæ¼”ç¤ºï¼‰\n",
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›\n",
    "    \n",
    "    å‚æ•°:\n",
    "    - Q: QueryçŸ©é˜µ (n, d_k)\n",
    "    - K: KeyçŸ©é˜µ (n, d_k)\n",
    "    - V: ValueçŸ©é˜µ (n, d_v)\n",
    "    - mask: æ©ç ï¼ˆå¯é€‰ï¼‰\n",
    "    \n",
    "    è¿”å›:\n",
    "    - output: æ³¨æ„åŠ›è¾“å‡º (n, d_v)\n",
    "    - attention_weights: æ³¨æ„åŠ›æƒé‡ (n, n)\n",
    "    \"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "    \n",
    "    # 1. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•° (ç›¸ä¼¼åº¦)\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n",
    "    \n",
    "    # 2. åº”ç”¨æ©ç ï¼ˆå¦‚æœæœ‰ï¼‰\n",
    "    if mask is not None:\n",
    "        scores = scores + (mask * -1e9)\n",
    "    \n",
    "    # 3. Softmaxå½’ä¸€åŒ–\n",
    "    attention_weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n",
    "    attention_weights = attention_weights / np.sum(attention_weights, axis=-1, keepdims=True)\n",
    "    \n",
    "    # 4. åŠ æƒæ±‚å’Œ\n",
    "    output = np.dot(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# ç¤ºä¾‹ï¼š3ä¸ªè¯çš„åºåˆ—\n",
    "seq_len = 3\n",
    "d_k = 4\n",
    "\n",
    "# éšæœºç”ŸæˆQ, K, Vï¼ˆå®é™…åº”ç”¨ä¸­æ˜¯é€šè¿‡çº¿æ€§å˜æ¢å¾—åˆ°ï¼‰\n",
    "np.random.seed(42)\n",
    "Q = np.random.randn(seq_len, d_k)\n",
    "K = np.random.randn(seq_len, d_k)\n",
    "V = np.random.randn(seq_len, d_k)\n",
    "\n",
    "# è®¡ç®—æ³¨æ„åŠ›\n",
    "output, attn_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(\"=== Self-Attention Example ===\")\n",
    "print(f\"\\nQuery shape: {Q.shape}\")\n",
    "print(f\"Key shape: {K.shape}\")\n",
    "print(f\"Value shape: {V.shape}\")\n",
    "print(f\"\\nAttention weights matrix:\\n{attn_weights}\")\n",
    "print(f\"\\nRow sums (should all be 1.0):\\n{attn_weights.sum(axis=1)}\")\n",
    "print(f\"\\nOutput shape: {output.shape}\")\n",
    "\n",
    "# å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "im = ax.imshow(attn_weights, cmap='YlOrRd', vmin=0, vmax=1)\n",
    "ax.set_xticks(range(seq_len))\n",
    "ax.set_yticks(range(seq_len))\n",
    "ax.set_xticklabels([f'Word{i+1}' for i in range(seq_len)])\n",
    "ax.set_yticklabels([f'Word{i+1}' for i in range(seq_len)])\n",
    "ax.set_xlabel('Key (Attended word)')\n",
    "ax.set_ylabel('Query (Current word)')\n",
    "ax.set_title('Attention Weights Heatmap')\n",
    "\n",
    "# åœ¨æ¯ä¸ªæ ¼å­ä¸­æ˜¾ç¤ºæ•°å€¼\n",
    "for i in range(seq_len):\n",
    "    for j in range(seq_len):\n",
    "        text = ax.text(j, i, f'{attn_weights[i, j]:.2f}',\n",
    "                      ha=\"center\", va=\"center\", color=\"black\", fontsize=12)\n",
    "\n",
    "plt.colorbar(im, ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Multi-Head Attention (å¤šå¤´æ³¨æ„åŠ›)\n",
    "\n",
    "#### åŠ¨æœºï¼šä¸ºä»€ä¹ˆéœ€è¦å¤šå¤´ï¼Ÿ\n",
    "\n",
    "å•ä¸ªæ³¨æ„åŠ›å¤´å¯èƒ½åªå…³æ³¨æŸä¸€ç§æ¨¡å¼,ä¾‹å¦‚:\n",
    "- è¯­æ³•å…³ç³»ï¼ˆä¸»è°“å®¾ï¼‰\n",
    "- è¯­ä¹‰ç›¸å…³æ€§\n",
    "- ä½ç½®æ¥è¿‘åº¦\n",
    "\n",
    "**å¤šå¤´æ³¨æ„åŠ›**å…è®¸æ¨¡å‹åŒæ—¶å…³æ³¨**å¤šç§ä¸åŒçš„æ¨¡å¼**ã€‚\n",
    "\n",
    "#### æ ¸å¿ƒæ€æƒ³\n",
    "\n",
    "å°†Q, K, Våˆ†æˆ $h$ ä¸ªå¤´,æ¯ä¸ªå¤´ç‹¬ç«‹è®¡ç®—attention,æœ€åæ‹¼æ¥:\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O\n",
    "$$\n",
    "\n",
    "å…¶ä¸­æ¯ä¸ªå¤´:\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(QW^Q_i, KW^K_i, VW^V_i)\n",
    "$$\n",
    "\n",
    "#### å‚æ•°è¯¦è§£\n",
    "\n",
    "- $d_{\\text{model}}$: æ¨¡å‹ç»´åº¦ï¼ˆå¦‚512ï¼‰\n",
    "- $h$: å¤´æ•°ï¼ˆå¦‚8ï¼‰\n",
    "- $d_k = d_v = d_{\\text{model}} / h$: æ¯ä¸ªå¤´çš„ç»´åº¦ï¼ˆå¦‚64ï¼‰\n",
    "\n",
    "#### ä¼˜åŠ¿\n",
    "\n",
    "1. **å¤šæ ·æ€§**: ä¸åŒçš„å¤´å­¦ä¹ ä¸åŒçš„æ³¨æ„åŠ›æ¨¡å¼\n",
    "2. **æ•ˆç‡**: æ€»å‚æ•°é‡ä¸å•å¤´ç›¸å½“\n",
    "3. **è¡¨è¾¾èƒ½åŠ›**: å¢å¼ºæ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›\n",
    "\n",
    "#### å…¸å‹é…ç½®\n",
    "\n",
    "| æ¨¡å‹ | $d_{\\text{model}}$ | $h$ | $d_k$ |\n",
    "|------|----------|-----|-------|\n",
    "| BERT-base | 768 | 12 | 64 |\n",
    "| GPT-3 | 12288 | 96 | 128 |\n",
    "| LLaMA-7B | 4096 | 32 | 128 |\n",
    "| Qwen-7B | 4096 | 32 | 128 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¤šå¤´æ³¨æ„åŠ›çš„å¯è§†åŒ–ç¤ºä¾‹\n",
    "def visualize_multi_head_attention():\n",
    "    \"\"\"\n",
    "    å¯è§†åŒ–å¤šå¤´æ³¨æ„åŠ›çš„æ¦‚å¿µ\n",
    "    \"\"\"\n",
    "    # æ¨¡æ‹Ÿä¸€ä¸ªå¥å­ \"I love natural language processing\"\n",
    "    words = [\"I\", \"love\", \"natural\", \"language\", \"processing\"]\n",
    "    n = len(words)\n",
    "    num_heads = 4\n",
    "    \n",
    "    # ä¸ºæ¯ä¸ªå¤´ç”Ÿæˆä¸åŒçš„æ³¨æ„åŠ›æ¨¡å¼\n",
    "    np.random.seed(42)\n",
    "    attention_patterns = []\n",
    "    \n",
    "    for i in range(num_heads):\n",
    "        # åˆ›å»ºä¸åŒçš„æ³¨æ„åŠ›æ¨¡å¼\n",
    "        attn = np.random.rand(n, n)\n",
    "        # å½’ä¸€åŒ–\n",
    "        attn = attn / attn.sum(axis=1, keepdims=True)\n",
    "        attention_patterns.append(attn)\n",
    "    \n",
    "    # å¯è§†åŒ–\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, (ax, attn) in enumerate(zip(axes, attention_patterns)):\n",
    "        im = ax.imshow(attn, cmap='YlOrRd', vmin=0, vmax=1)\n",
    "        ax.set_xticks(range(n))\n",
    "        ax.set_yticks(range(n))\n",
    "        ax.set_xticklabels(words, rotation=45)\n",
    "        ax.set_yticklabels(words)\n",
    "        ax.set_title(f'Head {idx+1}')\n",
    "        \n",
    "        # æ·»åŠ æ•°å€¼\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                text = ax.text(j, i, f'{attn[i, j]:.2f}',\n",
    "                              ha=\"center\", va=\"center\", \n",
    "                              color=\"black\" if attn[i, j] < 0.5 else \"white\",\n",
    "                              fontsize=8)\n",
    "        \n",
    "        plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "    \n",
    "    plt.suptitle('Multi-Head Attention: Different Heads Focus on Different Patterns', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Observations:\")\n",
    "    print(\"- Head 1 may focus on grammatical structure\")\n",
    "    print(\"- Head 2 may focus on semantic relationships\")\n",
    "    print(\"- Head 3 may focus on positional proximity\")\n",
    "    print(\"- Head 4 may focus on specific part-of-speech relationships\")\n",
    "\n",
    "visualize_multi_head_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Feed-Forward Network (å‰é¦ˆç½‘ç»œ)\n",
    "\n",
    "æ¯ä¸ªTransformerå±‚é™¤äº†Multi-Head Attention,è¿˜æœ‰ä¸€ä¸ª**Position-wise Feed-Forward Network**ã€‚\n",
    "\n",
    "#### ç»“æ„\n",
    "\n",
    "$$\n",
    "\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
    "$$\n",
    "\n",
    "å³ä¸¤å±‚å…¨è¿æ¥ç½‘ç»œ:\n",
    "\n",
    "```\n",
    "è¾“å…¥ (d_model)\n",
    "  â†“\n",
    "çº¿æ€§å˜æ¢ + ReLU (d_ff = 4 * d_model)\n",
    "  â†“\n",
    "çº¿æ€§å˜æ¢ (d_model)\n",
    "  â†“\n",
    "è¾“å‡º (d_model)\n",
    "```\n",
    "\n",
    "#### å…³é”®ç‰¹ç‚¹\n",
    "\n",
    "1. **Position-wise**: å¯¹æ¯ä¸ªä½ç½®ç‹¬ç«‹åº”ç”¨ç›¸åŒçš„FFN\n",
    "2. **æ‰©å¼ -å‹ç¼©**: ä¸­é—´å±‚é€šå¸¸æ˜¯4å€æ¨¡å‹ç»´åº¦\n",
    "3. **éçº¿æ€§**: å¼•å…¥éçº¿æ€§å˜æ¢èƒ½åŠ›\n",
    "\n",
    "#### ä½œç”¨\n",
    "\n",
    "- **Attentionè´Ÿè´£ä¿¡æ¯èšåˆ**: \"çœ‹å“ªé‡Œ\"\n",
    "- **FFNè´Ÿè´£ç‰¹å¾å˜æ¢**: \"åšä»€ä¹ˆ\"\n",
    "\n",
    "#### ç°ä»£å˜ä½“\n",
    "\n",
    "- **GELUæ¿€æ´»**: GPT, BERTä½¿ç”¨GELUæ›¿ä»£ReLU\n",
    "- **SwiGLU**: LLaMA, PaLMä½¿ç”¨\n",
    "  $$\\text{SwiGLU}(x) = \\text{Swish}(xW_1) \\odot (xW_2)$$\n",
    "- **Mixture of Experts (MoE)**: ç”¨å¤šä¸ªä¸“å®¶ç½‘ç»œæ›¿ä»£å•ä¸€FFNï¼ˆç¨åè¯¦ç»†è®²è§£ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Residual Connection (æ®‹å·®è¿æ¥)\n",
    "\n",
    "#### é—®é¢˜ï¼šæ·±å±‚ç½‘ç»œçš„æ¢¯åº¦æ¶ˆå¤±\n",
    "\n",
    "éšç€ç½‘ç»œåŠ æ·±,æ¢¯åº¦ä¼šé€å±‚è¡°å‡,å¯¼è‡´è®­ç»ƒå›°éš¾ã€‚\n",
    "\n",
    "#### è§£å†³æ–¹æ¡ˆï¼šæ®‹å·®è¿æ¥\n",
    "\n",
    "$$\n",
    "\\text{Output} = \\text{Sublayer}(x) + x\n",
    "$$\n",
    "\n",
    "ä¸ºå­å±‚ï¼ˆAttentionæˆ–FFNï¼‰çš„è¾“å‡ºåŠ ä¸Šè¾“å…¥æœ¬èº«ã€‚\n",
    "\n",
    "```\n",
    "    x â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚               â”‚\n",
    "    â†“               â”‚\n",
    "Sublayer            â”‚\n",
    "    â”‚               â”‚\n",
    "    â†“               â”‚\n",
    "    + â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    â”‚\n",
    "    â†“\n",
    "  Output\n",
    "```\n",
    "\n",
    "#### ä¼˜åŠ¿\n",
    "\n",
    "1. **æ¢¯åº¦ç›´é€š**: æ¢¯åº¦å¯ä»¥ç›´æ¥æµå›,ç¼“è§£æ¢¯åº¦æ¶ˆå¤±\n",
    "2. **æ’ç­‰æ˜ å°„**: æœ€åæƒ…å†µä¸‹,ç½‘ç»œå¯ä»¥å­¦ä¹ æ’ç­‰å˜æ¢\n",
    "3. **ç‰¹å¾å¤ç”¨**: ä¿ç•™æµ…å±‚ç‰¹å¾\n",
    "\n",
    "#### æ•°å­¦è§†è§’\n",
    "\n",
    "åå‘ä¼ æ’­æ—¶:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial \\text{Output}} \\left(\\frac{\\partial \\text{Sublayer}}{\\partial x} + I\\right)\n",
    "$$\n",
    "\n",
    "å³ä½¿ $\\frac{\\partial \\text{Sublayer}}{\\partial x} \\approx 0$,æ¢¯åº¦ä»ç„¶å¯ä»¥é€šè¿‡æ’ç­‰é¡¹ $I$ æµå›ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Layer Normalization (å±‚å½’ä¸€åŒ–)\n",
    "\n",
    "#### å½’ä¸€åŒ–çš„ä½œç”¨\n",
    "\n",
    "- ç¨³å®šè®­ç»ƒ\n",
    "- åŠ é€Ÿæ”¶æ•›\n",
    "- ç¼“è§£æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸\n",
    "\n",
    "#### Layer Norm vs Batch Norm\n",
    "\n",
    "| ç»´åº¦ | Batch Norm | Layer Norm |\n",
    "|------|-----------|------------|\n",
    "| **å½’ä¸€åŒ–ç»´åº¦** | æ‰¹æ¬¡ç»´åº¦ | ç‰¹å¾ç»´åº¦ |\n",
    "| **é€‚ç”¨åœºæ™¯** | CNN (å›ºå®šbatch size) | NLP (å˜é•¿åºåˆ—) |\n",
    "| **è®¡ç®—æ–¹å¼** | è·¨æ ·æœ¬,åŒä¸€ç‰¹å¾ | åŒä¸€æ ·æœ¬,è·¨ç‰¹å¾ |\n",
    "\n",
    "#### Layer Normå…¬å¼\n",
    "\n",
    "å¯¹äºè¾“å…¥ $x \\in \\mathbb{R}^{d}$:\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{1}{d}\\sum_{i=1}^{d} x_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma^2 = \\frac{1}{d}\\sum_{i=1}^{d} (x_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{LayerNorm}(x) = \\gamma \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta\n",
    "$$\n",
    "\n",
    "å…¶ä¸­:\n",
    "- $\\gamma, \\beta$ æ˜¯å¯å­¦ä¹ çš„ç¼©æ”¾å’Œå¹³ç§»å‚æ•°\n",
    "- $\\epsilon$ æ˜¯å°å¸¸æ•°(å¦‚1e-5)ï¼Œé˜²æ­¢é™¤é›¶\n",
    "\n",
    "#### Pre-LN vs Post-LN\n",
    "\n",
    "**åŸå§‹Transformer (Post-LN)**:\n",
    "```\n",
    "x â†’ Sublayer â†’ Add â†’ LayerNorm â†’ ...\n",
    "```\n",
    "\n",
    "**ç°ä»£æ¶æ„ (Pre-LN, GPT-2å)**:\n",
    "```\n",
    "x â†’ LayerNorm â†’ Sublayer â†’ Add â†’ ...\n",
    "```\n",
    "\n",
    "Pre-LNè®­ç»ƒæ›´ç¨³å®š,å°¤å…¶æ˜¯æ·±å±‚ç½‘ç»œã€‚\n",
    "\n",
    "#### å…¶ä»–å½’ä¸€åŒ–æ–¹æ³•\n",
    "\n",
    "- **RMSNorm**: LLaMAä½¿ç”¨,åªåšç¼©æ”¾ä¸åšå¹³ç§»\n",
    "  $$\\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{d}\\sum x_i^2 + \\epsilon}} \\cdot \\gamma$$\n",
    "- **DeepNorm**: é’ˆå¯¹è¶…æ·±æ¨¡å‹çš„å½’ä¸€åŒ–ç­–ç•¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer Normalizationçš„å®ç°\n",
    "def layer_norm(x, gamma, beta, eps=1e-5):\n",
    "    \"\"\"\n",
    "    Layer Normalization\n",
    "    \n",
    "    å‚æ•°:\n",
    "    - x: è¾“å…¥ (n, d)\n",
    "    - gamma: ç¼©æ”¾å‚æ•° (d,)\n",
    "    - beta: å¹³ç§»å‚æ•° (d,)\n",
    "    - eps: é˜²æ­¢é™¤é›¶çš„å°å¸¸æ•°\n",
    "    \"\"\"\n",
    "    # è®¡ç®—å‡å€¼å’Œæ–¹å·®ï¼ˆæ²¿ç‰¹å¾ç»´åº¦ï¼‰\n",
    "    mean = np.mean(x, axis=-1, keepdims=True)\n",
    "    var = np.var(x, axis=-1, keepdims=True)\n",
    "    \n",
    "    # å½’ä¸€åŒ–\n",
    "    x_norm = (x - mean) / np.sqrt(var + eps)\n",
    "    \n",
    "    # ç¼©æ”¾å’Œå¹³ç§»\n",
    "    out = gamma * x_norm + beta\n",
    "    \n",
    "    return out\n",
    "\n",
    "# ç¤ºä¾‹\n",
    "seq_len = 5\n",
    "d_model = 4\n",
    "\n",
    "# éšæœºè¾“å…¥\n",
    "x = np.random.randn(seq_len, d_model) * 10 + 5  # æ•…æ„ä½¿ç”¨è¾ƒå¤§çš„å‡å€¼å’Œæ–¹å·®\n",
    "\n",
    "# åˆå§‹åŒ–gammaå’Œbeta\n",
    "gamma = np.ones(d_model)\n",
    "beta = np.zeros(d_model)\n",
    "\n",
    "# åº”ç”¨Layer Norm\n",
    "x_normalized = layer_norm(x, gamma, beta)\n",
    "\n",
    "print(\"=== Layer Normalization Example ===\")\n",
    "print(f\"\\nOriginal input:\\n{x}\")\n",
    "print(f\"\\nOriginal input statistics:\")\n",
    "print(f\"- Row means: {np.mean(x, axis=1)}\")\n",
    "print(f\"- Row standard deviations: {np.std(x, axis=1)}\")\n",
    "\n",
    "print(f\"\\nAfter normalization:\\n{x_normalized}\")\n",
    "print(f\"\\nAfter normalization statistics:\")\n",
    "print(f\"- Row means: {np.mean(x_normalized, axis=1)}\")\n",
    "print(f\"- Row standard deviations: {np.std(x_normalized, axis=1)}\")\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.boxplot(x.T, labels=[f'Word{i+1}' for i in range(seq_len)])\n",
    "ax1.set_title('Distribution Before Normalization')\n",
    "ax1.set_ylabel('Value')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.boxplot(x_normalized.T, labels=[f'Word{i+1}' for i in range(seq_len)])\n",
    "ax2.set_title('Distribution After Normalization (Meanâ‰ˆ0, Stdâ‰ˆ1)')\n",
    "ax2.set_ylabel('Value')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Masking (æ©ç )\n",
    "\n",
    "æ©ç ç”¨äºæ§åˆ¶Attentionæœºåˆ¶ä¸­å“ªäº›ä½ç½®å¯ä»¥è¢«å…³æ³¨ã€‚\n",
    "\n",
    "#### 3.8.1 Padding Mask\n",
    "\n",
    "**é—®é¢˜**: æ‰¹å¤„ç†æ—¶,ä¸åŒæ ·æœ¬çš„åºåˆ—é•¿åº¦ä¸åŒ,éœ€è¦å¡«å……(padding)åˆ°ç›¸åŒé•¿åº¦ã€‚\n",
    "\n",
    "```\n",
    "æ ·æœ¬1: \"æˆ‘ çˆ± ä¸­å›½\" (é•¿åº¦3)\n",
    "æ ·æœ¬2: \"æ·±åº¦ å­¦ä¹  å¾ˆ æœ‰è¶£\" (é•¿åº¦4)\n",
    "\n",
    "å¡«å……å:\n",
    "æ ·æœ¬1: \"æˆ‘ çˆ± ä¸­å›½ [PAD]\"\n",
    "æ ·æœ¬2: \"æ·±åº¦ å­¦ä¹  å¾ˆ æœ‰è¶£\"\n",
    "```\n",
    "\n",
    "**è§£å†³**: Padding Maskç¡®ä¿æ¨¡å‹ä¸å…³æ³¨å¡«å……ä½ç½®ã€‚\n",
    "\n",
    "```python\n",
    "mask = (x != PAD_ID)  # [1, 1, 1, 0]\n",
    "scores = scores + (1 - mask) * -1e9  # å¡«å……ä½ç½®å¾—åˆ†â†’-âˆ\n",
    "```\n",
    "\n",
    "#### 3.8.2 Causal Mask (å› æœæ©ç )\n",
    "\n",
    "**ç”¨é€”**: è‡ªå›å½’ç”Ÿæˆï¼ˆå¦‚GPTï¼‰\n",
    "\n",
    "**è¦æ±‚**: ç”Ÿæˆç¬¬ $i$ ä¸ªè¯æ—¶,åªèƒ½çœ‹åˆ°å‰ $i-1$ ä¸ªè¯,ä¸èƒ½çœ‹åˆ°æœªæ¥çš„è¯ã€‚\n",
    "\n",
    "```\n",
    "è¾“å…¥: \"æˆ‘ çˆ± ä¸­å›½\"\n",
    "\n",
    "Attention mask:\n",
    "       æˆ‘  çˆ±  ä¸­  å›½\n",
    "  æˆ‘ [ 1   0   0   0 ]  â† \"æˆ‘\"åªèƒ½çœ‹è‡ªå·±\n",
    "  çˆ± [ 1   1   0   0 ]  â† \"çˆ±\"èƒ½çœ‹\"æˆ‘\"å’Œè‡ªå·±\n",
    "  ä¸­ [ 1   1   1   0 ]  â† \"ä¸­\"èƒ½çœ‹\"æˆ‘çˆ±\"å’Œè‡ªå·±\n",
    "  å›½ [ 1   1   1   1 ]  â† \"å›½\"èƒ½çœ‹æ‰€æœ‰å‰æ–‡\n",
    "```\n",
    "\n",
    "**å®ç°**: ä½¿ç”¨ä¸‹ä¸‰è§’çŸ©é˜µ\n",
    "\n",
    "```python\n",
    "causal_mask = np.tril(np.ones((seq_len, seq_len)))\n",
    "```\n",
    "\n",
    "#### 3.8.3 Cross-Attention Mask\n",
    "\n",
    "åœ¨Encoder-Decoderæ¶æ„ä¸­,Decoderå¯ä»¥å…³æ³¨Encoderçš„æ‰€æœ‰ä½ç½®,ä½†éœ€è¦Padding Maskã€‚\n",
    "\n",
    "#### æ©ç ç±»å‹æ€»ç»“\n",
    "\n",
    "| æ©ç ç±»å‹ | ç”¨é€” | åº”ç”¨åœºæ™¯ |\n",
    "|---------|------|----------|\n",
    "| **Padding Mask** | å¿½ç•¥å¡«å……ä½ç½® | æ‰€æœ‰Transformer |\n",
    "| **Causal Mask** | é˜²æ­¢çœ‹åˆ°æœªæ¥ | GPTç­‰è‡ªå›å½’æ¨¡å‹ |\n",
    "| **Custom Mask** | ç‰¹å®šä»»åŠ¡éœ€æ±‚ | å¦‚ç»“æ„åŒ–é¢„æµ‹ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ©ç çš„å¯è§†åŒ–\n",
    "seq_len = 6\n",
    "\n",
    "# 1. Causal Maskï¼ˆå› æœæ©ç ï¼‰\n",
    "causal_mask = np.tril(np.ones((seq_len, seq_len)))\n",
    "\n",
    "# 2. Padding Maskï¼ˆå‡è®¾æœ€åä¸¤ä¸ªä½ç½®æ˜¯paddingï¼‰\n",
    "padding_mask = np.ones((seq_len, seq_len))\n",
    "padding_mask[:, -2:] = 0  # æœ€åä¸¤åˆ—è®¾ä¸º0\n",
    "padding_mask[-2:, :] = 0  # æœ€åä¸¤è¡Œè®¾ä¸º0\n",
    "\n",
    "# 3. ç»„åˆæ©ç \n",
    "combined_mask = causal_mask * padding_mask\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "masks = [causal_mask, padding_mask, combined_mask]\n",
    "titles = ['Causal Mask\\n(Autoregressive)', 'Padding Mask\\n(Ignore Padding)', 'Combined Mask\\n(Combined)']\n",
    "\n",
    "for ax, mask, title in zip(axes, masks, titles):\n",
    "    im = ax.imshow(mask, cmap='RdYlGn', vmin=0, vmax=1)\n",
    "    ax.set_xticks(range(seq_len))\n",
    "    ax.set_yticks(range(seq_len))\n",
    "    ax.set_xticklabels([f'Pos{i}' for i in range(seq_len)], rotation=45)\n",
    "    ax.set_yticklabels([f'Pos{i}' for i in range(seq_len)])\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Key (Attended position)')\n",
    "    ax.set_ylabel('Query (Current position)')\n",
    "    \n",
    "    # æ·»åŠ æ•°å€¼\n",
    "    for i in range(seq_len):\n",
    "        for j in range(seq_len):\n",
    "            val = int(mask[i, j])\n",
    "            color = 'white' if val == 1 else 'black'\n",
    "            ax.text(j, i, str(val), ha=\"center\", va=\"center\", \n",
    "                   color=color, fontsize=10, weight='bold')\n",
    "    \n",
    "    plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "\n",
    "plt.suptitle('Different Types of Attention Masks (1=Visible, 0=Invisible)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Mask descriptions:\")\n",
    "print(\"- Causal Mask: Can only see current position and earlier positions\")\n",
    "print(\"- Padding Mask: Positions 4 and 5 are padding, all positions should not attend to them\")\n",
    "print(\"- Combined Mask: Combination of both mask types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mixture of Experts (MoE)\n",
    "\n",
    "MoEæ˜¯ä¸€ç§æ‰©å±•Transformerå®¹é‡çš„æŠ€æœ¯,åœ¨ä¸æˆæ¯”ä¾‹å¢åŠ è®¡ç®—é‡çš„æƒ…å†µä¸‹å¢åŠ å‚æ•°ã€‚\n",
    "\n",
    "### 4.1 æ ¸å¿ƒæ€æƒ³\n",
    "\n",
    "å°†Feed-Forward Networkæ›¿æ¢ä¸º**å¤šä¸ªä¸“å®¶ç½‘ç»œ**,æ¯ä¸ªtokenåªæ¿€æ´»éƒ¨åˆ†ä¸“å®¶ã€‚\n",
    "\n",
    "```\n",
    "ä¼ ç»ŸFFN:\n",
    "è¾“å…¥ â†’ FFN â†’ è¾“å‡º\n",
    "\n",
    "MoE:\n",
    "è¾“å…¥ â†’ Router â†’ é€‰æ‹©Top-Kä¸“å®¶ â†’ åŠ æƒç»„åˆ â†’ è¾“å‡º\n",
    "       â†“\n",
    "    [ Expert 1 ]\n",
    "    [ Expert 2 ]\n",
    "    [ Expert 3 ]\n",
    "    [   ...    ]\n",
    "    [ Expert N ]\n",
    "```\n",
    "\n",
    "### 4.2 æ•°å­¦è¡¨è¾¾\n",
    "\n",
    "å¯¹äºè¾“å…¥ $x$:\n",
    "\n",
    "1. **Routerè®¡ç®—æ¯ä¸ªä¸“å®¶çš„æƒé‡**:\n",
    "   $$G(x) = \\text{Softmax}(x \\cdot W_g)$$\n",
    "\n",
    "2. **é€‰æ‹©Top-Kä¸“å®¶** (å¦‚K=2):\n",
    "   $$\\text{TopK}(G(x)) \\rightarrow \\{e_1, e_2\\}$$\n",
    "\n",
    "3. **åŠ æƒç»„åˆä¸“å®¶è¾“å‡º**:\n",
    "   $$\\text{MoE}(x) = \\sum_{i \\in \\text{TopK}} G_i(x) \\cdot E_i(x)$$\n",
    "\n",
    "å…¶ä¸­ $E_i(x)$ æ˜¯ç¬¬ $i$ ä¸ªä¸“å®¶çš„è¾“å‡ºã€‚\n",
    "\n",
    "### 4.3 ä¼˜åŠ¿\n",
    "\n",
    "âœ… **å‚æ•°æ•ˆç‡**: 1.6Tå‚æ•°çš„MoEæ¨¡å‹è®¡ç®—é‡â‰ˆ100Bç¨ å¯†æ¨¡å‹  \n",
    "âœ… **ä¸“ä¸šåŒ–**: ä¸åŒä¸“å®¶å­¦ä¹ ä¸åŒæ¨¡å¼/é¢†åŸŸ  \n",
    "âœ… **æ‰©å±•æ€§**: å¯ä»¥è½»æ¾å¢åŠ ä¸“å®¶æ•°é‡\n",
    "\n",
    "### 4.4 æŒ‘æˆ˜\n",
    "\n",
    "âŒ **è´Ÿè½½å‡è¡¡**: æŸäº›ä¸“å®¶å¯èƒ½è¢«è¿‡åº¦ä½¿ç”¨  \n",
    "âŒ **è®­ç»ƒä¸ç¨³å®š**: Routeréœ€è¦ç‰¹æ®Šçš„æ­£åˆ™åŒ–  \n",
    "âŒ **é€šä¿¡å¼€é”€**: åˆ†å¸ƒå¼è®­ç»ƒæ—¶ä¸“å®¶é—´çš„é€šä¿¡\n",
    "\n",
    "### 4.5 è§£å†³æ–¹æ¡ˆ\n",
    "\n",
    "1. **è´Ÿè½½å‡è¡¡æŸå¤±**:\n",
    "   $$L_{\\text{balance}} = \\alpha \\cdot \\text{CV}(\\text{expert\\_load})$$\n",
    "   é¼“åŠ±å‡åŒ€åˆ†é…tokenåˆ°å„ä¸“å®¶\n",
    "\n",
    "2. **Expert Capacity**:\n",
    "   é™åˆ¶æ¯ä¸ªä¸“å®¶å¤„ç†çš„tokenæ•°é‡ä¸Šé™\n",
    "\n",
    "3. **Noiseæ³¨å…¥**:\n",
    "   åœ¨Routerä¸­åŠ å…¥å™ªå£°å¢åŠ æ¢ç´¢\n",
    "\n",
    "### 4.6 ä»£è¡¨æ¨¡å‹\n",
    "\n",
    "| æ¨¡å‹ | ä¸“å®¶æ•° | Top-K | æ€»å‚æ•° | æ¿€æ´»å‚æ•° |\n",
    "|------|-------|-------|--------|----------|\n",
    "| **Switch Transformer** | 128-2048 | 1 | 1.6T | ~10B |\n",
    "| **GLaM** | 64 | 2 | 1.2T | ~100B |\n",
    "| **Mixtral 8x7B** | 8 | 2 | 47B | ~13B |\n",
    "| **DeepSeek-V2** | 160 | 6 | 236B | ~21B |\n",
    "| **Qwen2-57B-A14B** | ~64 | variable | 57B | ~14B |\n",
    "\n",
    "### 4.7 MoEçš„æœªæ¥\n",
    "\n",
    "- **ç»†ç²’åº¦MoE**: ä¸“å®¶ç²’åº¦æ›´å°ï¼ˆå¦‚multi-headå±‚é¢ï¼‰\n",
    "- **åŠ¨æ€MoE**: æ ¹æ®ä»»åŠ¡åŠ¨æ€è°ƒæ•´ä¸“å®¶æ•°é‡\n",
    "- **å¤šæ¨¡æ€MoE**: ä¸åŒä¸“å®¶å¤„ç†ä¸åŒæ¨¡æ€ï¼ˆæ–‡æœ¬/å›¾åƒ/éŸ³é¢‘ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MoEæ¦‚å¿µå¯è§†åŒ–\n",
    "def visualize_moe():\n",
    "    \"\"\"\n",
    "    å¯è§†åŒ–MoEçš„ä¸“å®¶é€‰æ‹©è¿‡ç¨‹\n",
    "    \"\"\"\n",
    "    num_tokens = 8\n",
    "    num_experts = 8\n",
    "    top_k = 2\n",
    "    \n",
    "    # æ¨¡æ‹Ÿrouterä¸ºæ¯ä¸ªtokenè®¡ç®—çš„ä¸“å®¶æƒé‡\n",
    "    np.random.seed(42)\n",
    "    router_logits = np.random.randn(num_tokens, num_experts)\n",
    "    router_probs = np.exp(router_logits) / np.sum(np.exp(router_logits), axis=1, keepdims=True)\n",
    "    \n",
    "    # é€‰æ‹©Top-Kä¸“å®¶\n",
    "    topk_indices = np.argsort(router_probs, axis=1)[:, -top_k:]\n",
    "    \n",
    "    # åˆ›å»ºé€‰æ‹©çŸ©é˜µ\n",
    "    selection_matrix = np.zeros((num_tokens, num_experts))\n",
    "    for i in range(num_tokens):\n",
    "        for j in topk_indices[i]:\n",
    "            selection_matrix[i, j] = router_probs[i, j]\n",
    "    \n",
    "    # å¯è§†åŒ–\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # å·¦å›¾ï¼šå®Œæ•´çš„routeræ¦‚ç‡\n",
    "    im1 = ax1.imshow(router_probs, cmap='YlOrRd', aspect='auto')\n",
    "    ax1.set_xlabel('Expert ID')\n",
    "    ax1.set_ylabel('Token ID')\n",
    "    ax1.set_title('Router Weights for Each Token')\n",
    "    ax1.set_xticks(range(num_experts))\n",
    "    ax1.set_yticks(range(num_tokens))\n",
    "    plt.colorbar(im1, ax=ax1, label='Weight')\n",
    "    \n",
    "    # å³å›¾ï¼šTop-Ké€‰æ‹©å\n",
    "    im2 = ax2.imshow(selection_matrix, cmap='YlOrRd', aspect='auto')\n",
    "    ax2.set_xlabel('Expert ID')\n",
    "    ax2.set_ylabel('Token ID')\n",
    "    ax2.set_title(f'Top-{top_k} Expert Selection (Black=Not Selected)')\n",
    "    ax2.set_xticks(range(num_experts))\n",
    "    ax2.set_yticks(range(num_tokens))\n",
    "    plt.colorbar(im2, ax=ax2, label='Weight')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # ç»Ÿè®¡ä¸“å®¶è´Ÿè½½\n",
    "    expert_load = (selection_matrix > 0).sum(axis=0)\n",
    "    \n",
    "    # ç»˜åˆ¶è´Ÿè½½åˆ†å¸ƒ\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(range(num_experts), expert_load, color='steelblue')\n",
    "    plt.xlabel('Expert ID')\n",
    "    plt.ylabel('Number of Tokens Processed')\n",
    "    plt.title('Expert Load Distribution (Ideal: Uniform Distribution)')\n",
    "    plt.axhline(y=num_tokens*top_k/num_experts, color='r', linestyle='--', \n",
    "               label=f'Ideal Load = {num_tokens*top_k/num_experts:.1f}')\n",
    "    plt.xticks(range(num_experts))\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nMoE Statistics:\")\n",
    "    print(f\"- Total tokens: {num_tokens}\")\n",
    "    print(f\"- Number of experts: {num_experts}\")\n",
    "    print(f\"- Top-K: {top_k}\")\n",
    "    print(f\"- Percentage of activated parameters per token: {top_k/num_experts*100:.1f}%\")\n",
    "    print(f\"- Expert load: {expert_load}\")\n",
    "\n",
    "visualize_moe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transformerçš„ä¼˜åŠ¿ä¸å±€é™\n",
    "\n",
    "### 5.1 ä¼˜åŠ¿ âœ…\n",
    "\n",
    "1. **å¹¶è¡ŒåŒ–**\n",
    "   - ä¸åƒRNNéœ€è¦é¡ºåºå¤„ç†\n",
    "   - å¯ä»¥å……åˆ†åˆ©ç”¨GPU/TPUçš„å¹¶è¡Œèƒ½åŠ›\n",
    "   - è®­ç»ƒé€Ÿåº¦å¿«å‡ ä¸ªæ•°é‡çº§\n",
    "\n",
    "2. **é•¿è·ç¦»ä¾èµ–**\n",
    "   - Self-Attentionç›´æ¥å»ºæ¨¡ä»»æ„ä¸¤ä¸ªä½ç½®çš„å…³ç³»\n",
    "   - è·¯å¾„é•¿åº¦ä¸ºO(1)ï¼ŒRNNä¸ºO(n)\n",
    "   - æ›´å®¹æ˜“æ•æ‰é•¿è·ç¦»ä¾èµ–\n",
    "\n",
    "3. **å¯è§£é‡Šæ€§**\n",
    "   - Attentionæƒé‡å¯è§†åŒ–\n",
    "   - èƒ½çœ‹åˆ°æ¨¡å‹å…³æ³¨å“ªäº›ä¿¡æ¯\n",
    "\n",
    "4. **é€šç”¨æ€§**\n",
    "   - é€‚ç”¨äºå¤šç§ä»»åŠ¡å’Œæ¨¡æ€\n",
    "   - æˆä¸ºç»Ÿä¸€çš„æ·±åº¦å­¦ä¹ æ¶æ„\n",
    "\n",
    "5. **å¯æ‰©å±•æ€§**\n",
    "   - Scaling Law: æ€§èƒ½éšæ¨¡å‹å¤§å°ã€æ•°æ®é‡æŒç»­æå‡\n",
    "   - å·²æ‰©å±•åˆ°ä¸‡äº¿å‚æ•°çº§åˆ«\n",
    "\n",
    "### 5.2 å±€é™ âŒ\n",
    "\n",
    "1. **è®¡ç®—å¤æ‚åº¦**\n",
    "   - Self-Attention: $O(n^2 \\cdot d)$\n",
    "   - åºåˆ—é•¿åº¦ç¿»å€,è®¡ç®—é‡ç¿»4å€\n",
    "   - é™åˆ¶äº†å¤„ç†è¶…é•¿åºåˆ—çš„èƒ½åŠ›\n",
    "\n",
    "2. **å†…å­˜å ç”¨**\n",
    "   - éœ€è¦å­˜å‚¨ $n \\times n$ çš„attentionçŸ©é˜µ\n",
    "   - é•¿åºåˆ—æ—¶å†…å­˜çˆ†ç‚¸\n",
    "\n",
    "3. **ä½ç½®ç¼–ç **\n",
    "   - å¤–æ¨æ€§æœ‰é™ï¼ˆå¯¹è¶…é•¿åºåˆ—ï¼‰\n",
    "   - ä»åœ¨æ¢ç´¢æ›´å¥½çš„ç¼–ç æ–¹å¼\n",
    "\n",
    "4. **æ•°æ®éœ€æ±‚**\n",
    "   - éœ€è¦å¤§é‡æ•°æ®æ‰èƒ½å……åˆ†å‘æŒ¥æ€§èƒ½\n",
    "   - å°æ•°æ®é›†ä¸Šå¯èƒ½ä¸å¦‚RNN/CNN\n",
    "\n",
    "### 5.3 æ”¹è¿›æ–¹å‘\n",
    "\n",
    "#### é«˜æ•ˆAttention\n",
    "\n",
    "| æ–¹æ³• | å¤æ‚åº¦ | ä»£è¡¨æ¨¡å‹ |\n",
    "|------|--------|----------|\n",
    "| **Sparse Attention** | $O(n \\sqrt{n})$ | Sparse Transformer |\n",
    "| **Linear Attention** | $O(n)$ | Linformer, Performer |\n",
    "| **Sliding Window** | $O(n \\cdot w)$ | Longformer |\n",
    "| **Flash Attention** | $O(n^2)$ä½†IOä¼˜åŒ– | GPT-4, LLaMA |\n",
    "\n",
    "#### æ¶æ„åˆ›æ–°\n",
    "\n",
    "- **Hybrid Models**: ç»“åˆRNN/CNNå’ŒTransformerï¼ˆå¦‚RWKVï¼‰\n",
    "- **State Space Models**: Mamba, S4ç­‰çº¿æ€§å¤æ‚åº¦æ¨¡å‹\n",
    "- **Retrieval-Augmented**: ç»“åˆå¤–éƒ¨çŸ¥è¯†åº“,å¤„ç†è¶…é•¿æ–‡æ¡£\n",
    "\n",
    "### 5.4 Transformerå¤æ‚åº¦å¯¹æ¯”\n",
    "\n",
    "| æ¶æ„ | æ¯å±‚å¤æ‚åº¦ | é¡ºåºæ“ä½œ | æœ€å¤§è·¯å¾„é•¿åº¦ |\n",
    "|------|-----------|---------|-------------|\n",
    "| **Self-Attention** | $O(n^2 \\cdot d)$ | $O(1)$ | $O(1)$ |\n",
    "| **RNN** | $O(n \\cdot d^2)$ | $O(n)$ | $O(n)$ |\n",
    "| **CNN** | $O(k \\cdot n \\cdot d^2)$ | $O(1)$ | $O(\\log_k n)$ |\n",
    "\n",
    "å…¶ä¸­:\n",
    "- $n$: åºåˆ—é•¿åº¦\n",
    "- $d$: è¡¨ç¤ºç»´åº¦\n",
    "- $k$: å·ç§¯æ ¸å¤§å°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. æ€»ç»“ä¸å±•æœ›\n",
    "\n",
    "### 6.1 æ ¸å¿ƒçŸ¥è¯†å›é¡¾\n",
    "\n",
    "âœ… **Embedding**: å°†ç¦»æ•£tokenæ˜ å°„åˆ°è¿ç»­å‘é‡ç©ºé—´  \n",
    "âœ… **Positional Encoding**: æ³¨å…¥ä½ç½®ä¿¡æ¯  \n",
    "âœ… **Self-Attention**: å»ºæ¨¡åºåˆ—å†…éƒ¨çš„ä¾èµ–å…³ç³»  \n",
    "âœ… **Multi-Head Attention**: å¤šè§†è§’å…³æ³¨ä¸åŒæ¨¡å¼  \n",
    "âœ… **Feed-Forward Network**: ç‰¹å¾å˜æ¢  \n",
    "âœ… **Residual Connection**: ç¼“è§£æ¢¯åº¦æ¶ˆå¤±  \n",
    "âœ… **Layer Normalization**: ç¨³å®šè®­ç»ƒ  \n",
    "âœ… **Masking**: æ§åˆ¶ä¿¡æ¯æµåŠ¨  \n",
    "âœ… **MoE**: é«˜æ•ˆæ‰©å±•æ¨¡å‹å®¹é‡\n",
    "\n",
    "### 6.2 Transformerçš„æ¼”åŒ–\n",
    "\n",
    "```\n",
    "2017: Transformer (Encoder-Decoder)\n",
    "  â†“\n",
    "2018: BERT (Encoder-only, åŒå‘)\n",
    "      GPT-1 (Decoder-only, è‡ªå›å½’)\n",
    "  â†“\n",
    "2019: GPT-2, RoBERTa, T5, ALBERT\n",
    "  â†“\n",
    "2020: GPT-3, ViT (è§†è§‰), DETR (æ£€æµ‹)\n",
    "  â†“\n",
    "2021: Switch Transformer (MoE), CLIP (å¤šæ¨¡æ€)\n",
    "  â†“\n",
    "2022: PaLM, Flamingo, ChatGPT\n",
    "  â†“\n",
    "2023: GPT-4, LLaMA, Claude, PaLM-2\n",
    "  â†“\n",
    "2024: Mixtral, Gemini, Claude 3, Qwen2, DeepSeek-V2\n",
    "  â†“\n",
    "2025: ...\n",
    "```\n",
    "\n",
    "### 6.3 ä»Transformeråˆ°LLM\n",
    "\n",
    "ç°ä»£å¤§è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒéƒ½æ˜¯Transformer:\n",
    "\n",
    "**GPTç³»åˆ—** (Decoder-only):\n",
    "- è‡ªå›å½’è¯­è¨€å»ºæ¨¡\n",
    "- å› æœæ©ç \n",
    "- çº¯æ–‡æœ¬ç”Ÿæˆ\n",
    "\n",
    "**BERTç³»åˆ—** (Encoder-only):\n",
    "- æ©ç è¯­è¨€å»ºæ¨¡\n",
    "- åŒå‘ç¼–ç \n",
    "- æ–‡æœ¬ç†è§£ä»»åŠ¡\n",
    "\n",
    "**T5/BART** (Encoder-Decoder):\n",
    "- åºåˆ—åˆ°åºåˆ—\n",
    "- é€‚åˆç¿»è¯‘ã€æ‘˜è¦\n",
    "\n",
    "### 6.4 Transformerä¹‹å¤–\n",
    "\n",
    "è™½ç„¶Transformerä¸»å¯¼å½“å‰æ·±åº¦å­¦ä¹ ï¼Œä½†ä¹Ÿæœ‰æ›¿ä»£æ¶æ„:\n",
    "\n",
    "- **RWKV**: çº¿æ€§å¤æ‚åº¦,ç»“åˆRNNå’ŒTransformerä¼˜åŠ¿\n",
    "- **Mamba/S4**: çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œ$O(n)$å¤æ‚åº¦\n",
    "- **RetNet**: ä¿ç•™è®­ç»ƒå¹¶è¡Œæ€§å’Œæ¨ç†æ•ˆç‡\n",
    "\n",
    "### 6.5 å»¶ä¼¸é˜…è¯»\n",
    "\n",
    "ğŸ“š **å¿…è¯»è®ºæ–‡**:\n",
    "- \"Attention Is All You Need\" - Vaswani et al., 2017\n",
    "- \"BERT: Pre-training of Deep Bidirectional Transformers\" - Devlin et al., 2018\n",
    "- \"Language Models are Few-Shot Learners\" (GPT-3) - Brown et al., 2020\n",
    "\n",
    "ğŸŒ **æ¨èèµ„æº**:\n",
    "- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "- [The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/)\n",
    "- Hugging Face Transformersåº“æ–‡æ¡£\n",
    "\n",
    "---\n",
    "\n",
    "## è¯¾åæ€è€ƒ\n",
    "\n",
    "1. ä¸ºä»€ä¹ˆSelf-Attentionèƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰é•¿è·ç¦»ä¾èµ–ï¼Œè€ŒRNNåšä¸åˆ°ï¼Ÿ\n",
    "\n",
    "2. Multi-Head Attentionä¸­ï¼Œå¦‚æœæ‰€æœ‰çš„å¤´å­¦ä¹ åˆ°äº†ç›¸åŒçš„æ¨¡å¼ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿå¦‚ä½•é¿å…ï¼Ÿ\n",
    "\n",
    "3. Transformerçš„ $O(n^2)$ å¤æ‚åº¦åœ¨ä»€ä¹ˆåœºæ™¯ä¸‹ä¼šæˆä¸ºç“¶é¢ˆï¼Ÿæœ‰å“ªäº›è§£å†³æ–¹æ¡ˆï¼Ÿ\n",
    "\n",
    "4. MoEèƒ½å¦æ— é™å¢åŠ ä¸“å®¶æ•°é‡æ¥æå‡æ€§èƒ½ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ\n",
    "\n",
    "5. ä¸ºä»€ä¹ˆç°ä»£LLMå‡ ä¹éƒ½é€‰æ‹©Decoder-onlyæ¶æ„è€Œä¸æ˜¯Encoder-Decoderï¼Ÿ\n",
    "\n",
    "---\n",
    "\n",
    "**ä¸‹ä¸€è¯¾é¢„å‘Š**: ç¬¬4è¯¾ - ä¸»æµå¤§è¯­è¨€æ¨¡å‹  \n",
    "æˆ‘ä»¬å°†äº†è§£åŸºäºTransformeræ„å»ºçš„å„ç§å¤§è¯­è¨€æ¨¡å‹ï¼ˆQwenã€GPTã€LLaMAç­‰ï¼‰ï¼Œå®ƒä»¬çš„æ¶æ„å·®å¼‚ã€è®­ç»ƒæ–¹æ³•å’Œåº”ç”¨åœºæ™¯ï¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
