{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络与深度学习\n",
    "\n",
    "本教程将从零开始，带你理解神经网络的核心概念和深度学习的关键技术。\n",
    "\n",
    "## 目录\n",
    "\n",
    "**第一部分：基础构建块**\n",
    "- 1.1 感知机 (Perceptron)\n",
    "- 1.2 神经元 (Neuron)\n",
    "- 1.3 激活函数 (Activation Functions)\n",
    "- 1.4 网络架构 (Network Architecture)\n",
    "\n",
    "**第二部分：训练机制**\n",
    "- 2.1 前向传播 (Forward Propagation)\n",
    "- 2.2 损失函数 (Loss Functions)\n",
    "- 2.3 反向传播 (Backpropagation)\n",
    "\n",
    "**第三部分：为什么深度很重要**\n",
    "- 3.1 通用近似定理\n",
    "- 3.2 特征层次\n",
    "- 3.3 深度 vs 宽度\n",
    "\n",
    "**第四部分：训练深度网络的技巧**\n",
    "- 4.1 权重初始化 (Weight Initialization)\n",
    "- 4.2 优化与批处理 (Optimization & Batching)\n",
    "- 4.3 归一化 (Normalization)\n",
    "- 4.4 正则化与 Dropout (Regularization)\n",
    "\n",
    "**第五部分：实践**\n",
    "- 5.1 MNIST 手写数字识别\n",
    "- 5.2 练习题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 导入可视化模块\n",
    "import diagrams\n",
    "\n",
    "# 设置随机种子，保证结果可复现\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 第一部分：基础构建块\n",
    "\n",
    "## 1.1 感知机 (Perceptron)\n",
    "\n",
    "感知机是神经网络的最基本单元，由 Frank Rosenblatt 于 1957 年提出。\n",
    "\n",
    "### 核心思想\n",
    "\n",
    "感知机接收多个输入 $x_1, x_2, ..., x_n$，每个输入有一个权重 $w_1, w_2, ..., w_n$，计算加权和后通过阈值函数输出 0 或 1：\n",
    "\n",
    "$$\n",
    "y = \\begin{cases} \n",
    "1 & \\text{if } \\sum_{i} w_i x_i + b > 0 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### 几何直觉\n",
    "\n",
    "感知机本质上是在寻找一个**超平面**来分割数据：\n",
    "- 在 2D 空间中，超平面是一条**直线**\n",
    "- 在 3D 空间中，超平面是一个**平面**\n",
    "- 在 n 维空间中，超平面是 n-1 维的\n",
    "\n",
    "决策边界方程：$w_1 x_1 + w_2 x_2 + b = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化感知机决策边界\n",
    "# 权重 w = [1, 2]，偏置 b = -1\n",
    "# 决策边界：x1 + 2*x2 - 1 = 0\n",
    "\n",
    "# 生成一些示例数据点\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1], [0.5, 0.5], [1.5, 0.5], [0.5, 1.5]])\n",
    "y = np.array([0, 1, 0, 1, 0, 0, 1])  # 标签\n",
    "\n",
    "diagrams.plot_perceptron_boundary(w=np.array([1, 2]), b=-1, X=X, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 感知机的局限性\n",
    "\n",
    "感知机只能解决**线性可分**的问题。著名的 XOR 问题就无法用单个感知机解决：\n",
    "\n",
    "| $x_1$ | $x_2$ | XOR |\n",
    "|-------|-------|-----|\n",
    "| 0 | 0 | 0 |\n",
    "| 0 | 1 | 1 |\n",
    "| 1 | 0 | 1 |\n",
    "| 1 | 1 | 0 |\n",
    "\n",
    "这个局限性促使了多层网络（神经网络）的发展。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化 XOR 问题：为什么单个感知机无法解决\n",
    "diagrams.plot_xor_problem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.2 神经元 (Neuron)\n",
    "\n",
    "神经元是感知机的推广，主要区别在于：\n",
    "\n",
    "1. **激活函数可微分**：不再是硬阈值 (0/1)，而是平滑的函数如 sigmoid\n",
    "2. **输出连续**：可以是 0 到 1 之间的任意值\n",
    "\n",
    "### 神经元的计算过程\n",
    "\n",
    "$$z = \\sum_{i} w_i x_i + b = \\mathbf{w}^T \\mathbf{x} + b$$\n",
    "\n",
    "$$a = \\sigma(z)$$\n",
    "\n",
    "其中 $\\sigma$ 是激活函数（如 sigmoid）：\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "### 为什么需要可微分？\n",
    "\n",
    "可微分是训练神经网络的关键——我们需要计算梯度来更新权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示神经元的计算过程\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# 输入\n",
    "x = np.array([0.5, 0.8])\n",
    "w = np.array([0.4, 0.6])\n",
    "b = -0.5\n",
    "\n",
    "# 计算\n",
    "z = np.dot(w, x) + b  # 加权和\n",
    "a = sigmoid(z)        # 激活\n",
    "\n",
    "print(f\"输入 x = {x}\")\n",
    "print(f\"权重 w = {w}\")\n",
    "print(f\"偏置 b = {b}\")\n",
    "print(f\"加权和 z = w·x + b = {z:.4f}\")\n",
    "print(f\"激活输出 a = σ(z) = {a:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.3 激活函数 (Activation Functions)\n",
    "\n",
    "### 什么是激活函数？\n",
    "\n",
    "激活函数是应用在神经元输出上的**数学函数**，它将加权和转换为最终输出：\n",
    "\n",
    "$$z = \\mathbf{w}^T \\mathbf{x} + b \\quad \\text{(加权和，线性)}$$\n",
    "$$a = \\sigma(z) \\quad \\text{(激活函数，引入非线性)}$$\n",
    "\n",
    "名称来源于生物神经元——激活函数决定了神经元根据输入\"激活\"的程度。\n",
    "\n",
    "### 为什么需要激活函数？\n",
    "\n",
    "激活函数为神经网络引入**非线性**，这是神经网络能够学习复杂函数的关键。\n",
    "\n",
    "如果没有激活函数（或只用线性激活），多层网络等价于单层：\n",
    "\n",
    "$$y = W_2(W_1 x) = (W_2 W_1) x = W' x$$\n",
    "\n",
    "无论多少层，最终都是线性变换！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比较常见的激活函数\n",
    "diagrams.plot_activation_comparison(['sigmoid', 'tanh', 'relu', 'leaky_relu'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 激活函数总结\n",
    "\n",
    "| 函数 | 公式 | 输出范围 | 优点 | 缺点 |\n",
    "|------|------|----------|------|------|\n",
    "| Sigmoid | $\\frac{1}{1+e^{-x}}$ | (0, 1) | 输出概率解释 | 梯度消失、非零中心 |\n",
    "| Tanh | $\\frac{e^x-e^{-x}}{e^x+e^{-x}}$ | (-1, 1) | 零中心 | 梯度消失 |\n",
    "| ReLU | $\\max(0, x)$ | [0, ∞) | 计算简单、不饱和 | 死亡神经元 |\n",
    "| Leaky ReLU | $\\max(\\alpha x, x)$ | (-∞, ∞) | 解决死亡神经元 | 需要调参 $\\alpha$ |\n",
    "\n",
    "**现代实践**：隐藏层默认使用 ReLU 或其变体，输出层根据任务选择（分类用 softmax，回归用线性）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1.4 网络架构 (Network Architecture)\n",
    "\n",
    "神经网络由多个神经元分层组织而成。\n",
    "\n",
    "### 术语约定\n",
    "\n",
    "- **输入层 (Input Layer)**：接收原始数据，不做计算\n",
    "- **隐藏层 (Hidden Layers)**：中间层，进行特征变换\n",
    "- **输出层 (Output Layer)**：产生最终预测\n",
    "- **全连接层 (Fully Connected)**：每个神经元与上一层所有神经元相连"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化一个简单的神经网络：2输入 -> 4隐藏 -> 3隐藏 -> 1输出\n",
    "diagrams.plot_network(layers=[2, 4, 3, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 符号约定 (Notation)\n",
    "\n",
    "我们采用 Michael Nielsen 的符号系统：\n",
    "\n",
    "- $L$：网络总层数\n",
    "- $n^{[l]}$：第 $l$ 层的神经元数量\n",
    "- $W^{[l]}$：第 $l$ 层的权重矩阵，形状为 $(n^{[l]}, n^{[l-1]})$\n",
    "- $b^{[l]}$：第 $l$ 层的偏置向量，形状为 $(n^{[l]}, 1)$\n",
    "- $z^{[l]}$：第 $l$ 层的加权输入\n",
    "- $a^{[l]}$：第 $l$ 层的激活输出\n",
    "\n",
    "### 为什么 W 的形状是 $(n^{[l]}, n^{[l-1]})$？\n",
    "\n",
    "为了让矩阵乘法 $z^{[l]} = W^{[l]} a^{[l-1]} + b^{[l]}$ 正确工作：\n",
    "\n",
    "```\n",
    "a^[l-1]:  (n^[l-1], 1)    ← 上一层的输出（列向量）\n",
    "W^[l]:    (n^[l], n^[l-1]) ← 权重矩阵\n",
    "z^[l]:    (n^[l], 1)       ← 当前层的输入（列向量）\n",
    "\n",
    "矩阵乘法: (n^[l], n^[l-1]) @ (n^[l-1], 1) = (n^[l], 1) ✓\n",
    "```\n",
    "\n",
    "**直觉理解**：$W$ 的每一**行**包含一个神经元的所有输入权重。\n",
    "\n",
    "以上面的网络 `[2, 4, 3, 1]` 为例：\n",
    "\n",
    "| 层 | $W$ 形状 | $b$ 形状 | 含义 |\n",
    "|----|----------|----------|------|\n",
    "| 1 | (4, 2) | (4, 1) | 2 个输入 → 4 个神经元 |\n",
    "| 2 | (3, 4) | (3, 1) | 4 个输入 → 3 个神经元 |\n",
    "| 3 | (1, 3) | (1, 1) | 3 个输入 → 1 个输出 |\n",
    "\n",
    "**维度关系**：\n",
    "$$z^{[l]} = W^{[l]} a^{[l-1]} + b^{[l]}$$\n",
    "$$a^{[l]} = \\sigma(z^{[l]})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化单样本的维度流动\n",
    "# 以第一层为例：2 个输入 → 4 个神经元\n",
    "diagrams.plot_layer_dimensions(n_in=2, n_out=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 第二部分：训练机制\n",
    "\n",
    "## 2.1 前向传播 (Forward Propagation)\n",
    "\n",
    "前向传播是从输入到输出的计算过程。\n",
    "\n",
    "### 单样本前向传播\n",
    "\n",
    "对于一个 $L$ 层网络：\n",
    "\n",
    "1. 输入：$a^{[0]} = x$\n",
    "2. 对每一层 $l = 1, 2, ..., L$：\n",
    "   - $z^{[l]} = W^{[l]} a^{[l-1]} + b^{[l]}$\n",
    "   - $a^{[l]} = \\sigma(z^{[l]})$\n",
    "3. 输出：$\\hat{y} = a^{[L]}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现前向传播\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    前向传播\n",
    "    \n",
    "    参数:\n",
    "        X: 输入数据 (n_features, n_samples)\n",
    "        parameters: 字典 {'W1': ..., 'b1': ..., 'W2': ..., 'b2': ...}\n",
    "    \n",
    "    返回:\n",
    "        A: 最终输出\n",
    "        cache: 中间值缓存（用于反向传播）\n",
    "    \"\"\"\n",
    "    cache = {'A0': X}\n",
    "    A = X\n",
    "    L = len(parameters) // 2  # 层数\n",
    "    for l in range(1, L + 1):\n",
    "        W = parameters[f'W{l}']\n",
    "        b = parameters[f'b{l}']\n",
    "        Z = np.dot(W, A) + b\n",
    "        A = sigmoid(Z)\n",
    "        cache[f'Z{l}'] = Z\n",
    "        cache[f'A{l}'] = A\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "# 示例：2层网络\n",
    "params = {\n",
    "    'W1': np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]),  # (3, 2)\n",
    "    'b1': np.array([[0], [0], [0]]),                        # (3, 1)\n",
    "    'W2': np.array([[0.7, 0.8, 0.9]]),                       # (1, 3)\n",
    "    'b2': np.array([[0]])                                    # (1, 1)\n",
    "}\n",
    "\n",
    "X = np.array([[1], [2]])  # 单个样本 (2, 1)\n",
    "output, cache = forward_propagation(X, params)\n",
    "print(f\"输入 X:\\n{X}\")\n",
    "print(f\"\\n隐藏层输出 A1:\\n{cache['A1']}\")\n",
    "print(f\"\\n最终输出:\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 向量化：批量处理\n",
    "\n",
    "实际训练中，我们同时处理多个样本。将 $m$ 个样本堆叠成矩阵：\n",
    "\n",
    "$$X = [x^{(1)}, x^{(2)}, ..., x^{(m)}] \\in \\mathbb{R}^{n \\times m}$$\n",
    "\n",
    "前向传播公式不变，矩阵运算自动处理批量计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化批量处理的维度流动\n",
    "# 每个矩阵的列数 m 代表批量中的样本数\n",
    "diagrams.plot_batch_dimensions(layers=[2, 4, 3, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.2 损失函数 (Loss Functions)\n",
    "\n",
    "损失函数衡量预测与真实值的差距，是优化的目标。\n",
    "\n",
    "### 均方误差 (MSE) - 回归任务\n",
    "\n",
    "$$L = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})^2$$\n",
    "\n",
    "### 交叉熵 (Cross-Entropy) - 分类任务\n",
    "\n",
    "二分类：\n",
    "$$L = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} \\log(\\hat{y}^{(i)}) + (1-y^{(i)}) \\log(1-\\hat{y}^{(i)})]$$\n",
    "\n",
    "### 为什么分类用交叉熵而不是 MSE？\n",
    "\n",
    "1. **梯度特性**：MSE 在 sigmoid 输出接近 0 或 1 时梯度很小，学习慢\n",
    "2. **概率解释**：交叉熵直接优化预测概率分布\n",
    "3. **数值稳定**：交叉熵与 softmax 结合时数值更稳定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化 MSE vs 交叉熵损失曲线\n",
    "# 注意交叉熵对错误预测的惩罚更重（-log 曲线趋向无穷）\n",
    "diagrams.plot_loss_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化损失曲面\n",
    "diagrams.plot_loss_landscape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2.3 反向传播 (Backpropagation)\n",
    "\n",
    "### 从损失到学习\n",
    "\n",
    "前向传播给了我们预测值 $\\hat{y}$，损失函数告诉我们预测有多\"错\"。\n",
    "\n",
    "**现在的问题是**：如何调整权重 $W$ 和偏置 $b$，让损失变小？\n",
    "\n",
    "答案是**梯度下降**：沿着损失下降最快的方向更新参数。\n",
    "\n",
    "$$W \\leftarrow W - \\alpha \\frac{\\partial L}{\\partial W}$$\n",
    "\n",
    "**核心挑战**：神经网络有成千上万个参数，如何高效计算每个参数的梯度 $\\frac{\\partial L}{\\partial W}$？\n",
    "\n",
    "这就是反向传播要解决的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 链式法则：反向传播的数学基础\n",
    "\n",
    "反向传播的核心是微积分中的**链式法则**。\n",
    "\n",
    "**简单例子**：如果 $y = f(u)$ 且 $u = g(x)$，则：\n",
    "\n",
    "$$\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}$$\n",
    "\n",
    "**具体计算**：设 $y = (2x + 1)^3$\n",
    "\n",
    "令 $u = 2x + 1$，则 $y = u^3$\n",
    "\n",
    "$$\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx} = 3u^2 \\cdot 2 = 6(2x+1)^2$$\n",
    "\n",
    "**多变量情况**：如果 $L$ 依赖于多个中间变量，梯度沿着所有路径累加：\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial x} = \\sum_i \\frac{\\partial L}{\\partial u_i} \\cdot \\frac{\\partial u_i}{\\partial x}$$\n",
    "\n",
    "这就是梯度如何在神经网络中\"反向流动\"的原理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化计算图：单个神经元 y = sigmoid(w*x + b)\n",
    "diagrams.plot_computational_graph('y = sigmoid(w*x + b)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 应用链式法则：单神经元的梯度推导\n",
    "\n",
    "现在让我们用链式法则推导上面计算图中每个参数的梯度。\n",
    "\n",
    "**设置**：单神经元 $y = \\sigma(wx + b)$，损失 $L = \\frac{1}{2}(y - t)^2$（MSE）\n",
    "\n",
    "**目标**：求 $\\frac{\\partial L}{\\partial w}$ 和 $\\frac{\\partial L}{\\partial b}$\n",
    "\n",
    "---\n",
    "\n",
    "**Step 1: 识别计算链路**\n",
    "\n",
    "$$x \\xrightarrow{w} z = wx + b \\xrightarrow{\\sigma} y = \\sigma(z) \\xrightarrow{L} L = \\frac{1}{2}(y-t)^2$$\n",
    "\n",
    "**Step 2: 从输出往回推（链式法则）**\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w}$$\n",
    "\n",
    "**Step 3: 计算每一项**\n",
    "\n",
    "| 导数 | 计算 | 结果 |\n",
    "|------|------|------|\n",
    "| $\\frac{\\partial L}{\\partial y}$ | $\\frac{\\partial}{\\partial y}\\frac{1}{2}(y-t)^2$ | $y - t$ |\n",
    "| $\\frac{\\partial y}{\\partial z}$ | $\\frac{\\partial}{\\partial z}\\sigma(z)$ | $\\sigma'(z)$ |\n",
    "| $\\frac{\\partial z}{\\partial w}$ | $\\frac{\\partial}{\\partial w}(wx + b)$ | $x$ |\n",
    "\n",
    "**Step 4: 组合**\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w} = (y - t) \\cdot \\sigma'(z) \\cdot x$$\n",
    "\n",
    "同理：\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b} = (y - t) \\cdot \\sigma'(z) \\cdot 1 = (y - t) \\cdot \\sigma'(z)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义误差项 $\\delta$\n",
    "\n",
    "上面的推导中$\\frac{\\partial L}{\\partial z}$：\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial z} = (y-t) \\cdot \\sigma'(z)$$\n",
    "\n",
    "我们给它一个名字——**误差项**（error term）：\n",
    "\n",
    "$$\\delta = \\frac{\\partial L}{\\partial z}$$\n",
    "\n",
    "**直觉理解**：$\\delta$ 表示\"如果 $z$ 变化一点点，损失会变化多少\"。\n",
    "\n",
    "- $|\\delta|$ 大 → 这个神经元对损失影响大 → 需要大幅调整\n",
    "- $|\\delta|$ 小 → 这个神经元对损失影响小 → 微调即可\n",
    "\n",
    "有了 $\\delta$，梯度公式变得简洁：\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w} = \\delta \\cdot x$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b} = \\delta$$\n",
    "\n",
    "**关键洞察**：一旦知道了 $\\delta$，计算参数梯度就很简单——只需乘以对应的输入！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 反向传播的四个核心方程\n",
    "\n",
    "上面我们推导了单神经元的情况。对于多层网络，核心思想相同：\n",
    "\n",
    "1. 每层都有误差项 $\\delta^{[l]}$\n",
    "2. 误差从输出层**反向传播**到前面的层\n",
    "3. 每层的参数梯度 = 该层误差 × 该层输入\n",
    "\n",
    "---\n",
    "\n",
    "**方程 1：输出层误差**\n",
    "\n",
    "$$\\delta^{[L]} = \\nabla_a L \\odot \\sigma'(z^{[L]})$$\n",
    "\n",
    "- 从损失函数对输出的梯度开始\n",
    "- 乘以激活函数的导数（与单神经元相同）\n",
    "\n",
    "---\n",
    "\n",
    "**方程 2：误差反向传播**\n",
    "\n",
    "$$\\delta^{[l]} = (W^{[l+1]})^T \\delta^{[l+1]} \\odot \\sigma'(z^{[l]})$$\n",
    "\n",
    "- 下一层的误差通过权重矩阵**转置**反向传播\n",
    "- 乘以当前层激活函数的导数\n",
    "- 这就是\"反向传播\"名称的由来！\n",
    "\n",
    "---\n",
    "\n",
    "**方程 3：偏置梯度**\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b^{[l]}} = \\delta^{[l]}$$\n",
    "\n",
    "- 偏置的梯度就是误差项本身（与单神经元相同：$\\frac{\\partial L}{\\partial b} = \\delta$）\n",
    "\n",
    "---\n",
    "\n",
    "**方程 4：权重梯度**\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W^{[l]}} = \\delta^{[l]} (a^{[l-1]})^T$$\n",
    "\n",
    "- 权重梯度 = 当前层误差 × 上一层激活的转置\n",
    "- 与单神经元相同：$\\frac{\\partial L}{\\partial w} = \\delta \\cdot x$，只是扩展到矩阵形式\n",
    "\n",
    "---\n",
    "\n",
    "**记忆技巧**：\n",
    "- 方程 1-2 计算误差 $\\delta$（从输出到输入）\n",
    "- 方程 3-4 用误差计算参数梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手算反向传播示例\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "# 参数\n",
    "w1, b1 = 0.5, 0.0\n",
    "w2, b2 = 0.8, 0.0\n",
    "x, y = 1.0, 1.0\n",
    "\n",
    "# ========== 前向传播 ==========\n",
    "print(\"=\"*50)\n",
    "print(\"前向传播\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "z1 = w1 * x + b1\n",
    "a1 = sigmoid(z1)\n",
    "print(f\"隐藏层: z1 = {w1}×{x} + {b1} = {z1}\")\n",
    "print(f\"        a1 = σ({z1}) = {a1:.4f}\")\n",
    "\n",
    "z2 = w2 * a1 + b2\n",
    "a2 = sigmoid(z2)  # 这是预测值 ŷ\n",
    "print(f\"输出层: z2 = {w2}×{a1:.4f} + {b2} = {z2:.4f}\")\n",
    "print(f\"        a2 = σ({z2:.4f}) = {a2:.4f}  ← 预测值\")\n",
    "\n",
    "loss = 0.5 * (a2 - y) ** 2\n",
    "print(f\"\\n损失: L = ½({a2:.4f} - {y})² = {loss:.4f}\")\n",
    "\n",
    "# ========== 反向传播 ==========\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"反向传播\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 输出层误差 (方程1)\n",
    "dL_da2 = a2 - y  # MSE 对 a2 的导数\n",
    "delta2 = dL_da2 * sigmoid_derivative(z2)\n",
    "print(f\"输出层误差: δ2 = (a2-y) × σ'(z2) = {dL_da2:.4f} × {sigmoid_derivative(z2):.4f} = {delta2:.4f}\")\n",
    "\n",
    "# 隐藏层误差 (方程2)\n",
    "delta1 = w2 * delta2 * sigmoid_derivative(z1)\n",
    "print(f\"隐藏层误差: δ1 = w2 × δ2 × σ'(z1) = {w2} × {delta2:.4f} × {sigmoid_derivative(z1):.4f} = {delta1:.4f}\")\n",
    "\n",
    "# 参数梯度 (方程3, 4)\n",
    "print(f\"\\n参数梯度:\")\n",
    "dL_dw2 = delta2 * a1\n",
    "dL_db2 = delta2\n",
    "print(f\"  ∂L/∂w2 = δ2 × a1 = {delta2:.4f} × {a1:.4f} = {dL_dw2:.4f}\")\n",
    "print(f\"  ∂L/∂b2 = δ2 = {dL_db2:.4f}\")\n",
    "\n",
    "dL_dw1 = delta1 * x\n",
    "dL_db1 = delta1\n",
    "print(f\"  ∂L/∂w1 = δ1 × x  = {delta1:.4f} × {x} = {dL_dw1:.4f}\")\n",
    "print(f\"  ∂L/∂b1 = δ1 = {dL_db1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实用技巧：交叉熵 + Sigmoid 的简化\n",
    "\n",
    "在实际应用中，二分类常用交叉熵损失 + sigmoid 激活。\n",
    "\n",
    "令人惊喜的是，输出层误差 $\\delta^{[L]}$ 会变得非常简洁！\n",
    "\n",
    "**推导**：\n",
    "\n",
    "交叉熵损失：$L = -[y\\log(a) + (1-y)\\log(1-a)]$\n",
    "\n",
    "对 $a$ 求导：$\\frac{\\partial L}{\\partial a} = -\\frac{y}{a} + \\frac{1-y}{1-a} = \\frac{a-y}{a(1-a)}$\n",
    "\n",
    "而 sigmoid 的导数：$\\sigma'(z) = a(1-a)$\n",
    "\n",
    "所以：\n",
    "\n",
    "$$\\delta^{[L]} = \\frac{\\partial L}{\\partial a} \\cdot \\sigma'(z) = \\frac{a-y}{a(1-a)} \\cdot a(1-a) = a - y$$\n",
    "\n",
    "**结论**：使用交叉熵 + sigmoid 时，输出层误差就是 **预测值减去真实值**！\n",
    "\n",
    "$$\\delta^{[L]} = \\hat{y} - y$$\n",
    "\n",
    "这不仅计算简单，而且梯度不会消失（相比 MSE + sigmoid）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 向量化的反向传播实现\n",
    "\n",
    "def backward_propagation(Y, cache, parameters):\n",
    "    \"\"\"\n",
    "    反向传播 (向量化版本)\n",
    "    \n",
    "    参数:\n",
    "        Y: 真实标签 (n_out, m)\n",
    "        cache: 前向传播的缓存 {'A0': X, 'Z1': ..., 'A1': ..., ...}\n",
    "        parameters: 网络参数 {'W1': ..., 'b1': ..., ...}\n",
    "    \n",
    "    返回:\n",
    "        grads: 梯度字典 {'dW1': ..., 'db1': ..., ...}\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(parameters) // 2  # 网络层数\n",
    "    m = Y.shape[1]            # 样本数量\n",
    "    \n",
    "    # 获取输出层激活值\n",
    "    AL = cache[f'A{L}']\n",
    "    \n",
    "    # 方程1: 输出层误差 (使用交叉熵损失，简化为 a - y)\n",
    "    dZ = AL - Y  # δ^[L] = a^[L] - y\n",
    "    \n",
    "    # 从输出层反向遍历到第一层\n",
    "    for l in reversed(range(1, L + 1)):\n",
    "        A_prev = cache[f'A{l-1}']\n",
    "        \n",
    "        # 方程4: 权重梯度 ∂L/∂W = δ @ A_prev^T / m\n",
    "        grads[f'dW{l}'] = (1/m) * np.dot(dZ, A_prev.T)\n",
    "        \n",
    "        # 方程3: 偏置梯度 ∂L/∂b = mean(δ)\n",
    "        grads[f'db{l}'] = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "        \n",
    "        # 方程2: 误差反向传播到上一层 (如果不是第一层)\n",
    "        if l > 1:\n",
    "            W = parameters[f'W{l}']\n",
    "            Z_prev = cache[f'Z{l-1}']\n",
    "            # δ^[l-1] = W^T @ δ^[l] ⊙ σ'(z^[l-1])\n",
    "            dA_prev = np.dot(W.T, dZ)\n",
    "            dZ = dA_prev * sigmoid_derivative(Z_prev)\n",
    "    \n",
    "    return grads\n",
    "\n",
    "# 测试：使用之前定义的前向传播\n",
    "Y = np.array([[1]])  # 真实标签\n",
    "grads = backward_propagation(Y, cache, params)\n",
    "\n",
    "print(\"计算得到的梯度:\")\n",
    "for key, value in sorted(grads.items()):\n",
    "    print(f\"  {key}: shape {value.shape}, values = {value.flatten()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度下降更新\n",
    "\n",
    "计算出梯度后，使用梯度下降更新参数：\n",
    "\n",
    "$$W^{[l]} = W^{[l]} - \\alpha \\frac{\\partial L}{\\partial W^{[l]}}$$\n",
    "\n",
    "$$b^{[l]} = b^{[l]} - \\alpha \\frac{\\partial L}{\\partial b^{[l]}}$$\n",
    "\n",
    "其中 $\\alpha$ 是学习率 (learning rate)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化梯度下降在损失曲面上的轨迹\n",
    "history = {\n",
    "    'w': [1.8, 1.5, 1.2, 0.9, 0.6, 0.4, 0.2, 0.1, 0.05, 0.02],\n",
    "    'b': [1.5, 1.2, 0.9, 0.7, 0.5, 0.35, 0.2, 0.1, 0.05, 0.02],\n",
    "    'loss': [3.0, 2.2, 1.5, 1.0, 0.6, 0.35, 0.2, 0.1, 0.05, 0.02]\n",
    "}\n",
    "diagrams.plot_gradient_descent_path(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 反向传播总结\n",
    "\n",
    "**完整训练循环**：\n",
    "\n",
    "```\n",
    "重复直到收敛:\n",
    "    1. 前向传播: 计算预测值和损失\n",
    "    2. 反向传播: 计算所有参数的梯度\n",
    "    3. 参数更新: W ← W - α × ∂L/∂W\n",
    "```\n",
    "\n",
    "**核心思想**：\n",
    "- 误差从输出层**反向流动**到输入层\n",
    "- 每层的误差 $\\delta$ 告诉我们该层对总损失的\"贡献\"\n",
    "- 梯度 = 误差 × 输入，用于更新权重\n",
    "\n",
    "**为什么高效**：\n",
    "- 共享中间计算结果\n",
    "- 一次前向 + 一次反向 = 所有梯度\n",
    "- 时间复杂度与前向传播相同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 第三部分：为什么深度很重要\n",
    "\n",
    "## 3.1 通用近似定理 (Universal Approximation Theorem)\n",
    "\n",
    "**定理**：一个单隐藏层的前馈神经网络，只要有足够多的神经元，可以近似任意连续函数。\n",
    "\n",
    "### 那为什么还需要深度？\n",
    "\n",
    "通用近似定理只保证**存在性**，没有说明：\n",
    "1. 需要多少神经元？（可能是指数级的）\n",
    "2. 能否通过训练找到这些参数？\n",
    "\n",
    "深度网络的优势在于**效率**和**泛化**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 特征层次 (Feature Hierarchy)\n",
    "\n",
    "深度网络自动学习**层次化的特征表示**——这是深度学习最强大的特性之一。\n",
    "\n",
    "### 以图像识别为例\n",
    "\n",
    "| 层级 | 学到的特征 | 抽象程度 |\n",
    "|------|------------|----------|\n",
    "| 第 1 层 | 边缘、梯度、颜色斑块 | 低（像素级） |\n",
    "| 第 2 层 | 纹理、角点、简单形状 | ↓ |\n",
    "| 第 3 层 | 部件（眼睛、轮子、窗户） | ↓ |\n",
    "| 第 4 层 | 整体概念（人脸、汽车、房子） | 高（语义级） |\n",
    "\n",
    "### 为什么层次化很重要？\n",
    "\n",
    "1. **复用性**：低层特征（边缘）可被多个高层概念共享\n",
    "2. **组合爆炸**：$n$ 个基础特征可组合成 $2^n$ 种高级概念\n",
    "3. **泛化能力**：学到的特征可迁移到相关任务\n",
    "\n",
    "这就是为什么预训练模型（如 ImageNet 上训练的网络）能迁移到其他视觉任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 深度 vs 宽度\n",
    "\n",
    "### 核心问题\n",
    "\n",
    "给定相同的参数预算，是选择**深而窄**还是**浅而宽**的网络？\n",
    "\n",
    "### 参数效率\n",
    "\n",
    "| 网络类型 | 架构示例 | 参数量 | 表达能力 |\n",
    "|----------|----------|--------|----------|\n",
    "| 浅而宽 | [3, 16, 1] | 81 | 可以，但效率低 |\n",
    "| 深而窄 | [3, 4, 4, 4, 1] | 61 | 同样甚至更好 |\n",
    "\n",
    "**关键洞察**：深度网络通过**函数组合**实现复杂映射，而浅层网络需要在单层\"硬编码\"所有复杂性。\n",
    "\n",
    "### 类比：编程思维\n",
    "\n",
    "```python\n",
    "# 浅而宽：一个巨大的 if-else（指数级条件）\n",
    "if condition_1 and condition_2 and condition_3 ...\n",
    "\n",
    "# 深而窄：函数组合（多项式级）\n",
    "result = f3(f2(f1(x)))\n",
    "```\n",
    "\n",
    "### 电路复杂度理论\n",
    "\n",
    "计算某些函数（如多层 XOR）：\n",
    "- 浅层电路：需要指数级的门\n",
    "- 深层电路：只需要多项式级的门\n",
    "\n",
    "### 实践建议\n",
    "\n",
    "- **深度优先**：深度通常比宽度更重要\n",
    "- **注意瓶颈**：太深会导致梯度消失/爆炸\n",
    "- **现代解决方案**：残差连接 (ResNet)、归一化层使深层训练成为可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化深度 vs 宽度的参数效率对比\n",
    "diagrams.plot_depth_vs_width()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 第四部分：训练深度网络的技巧\n",
    "\n",
    "深度网络难以训练的主要问题：\n",
    "1. **梯度消失/爆炸**\n",
    "2. **内部协变量偏移**\n",
    "3. **过拟合**\n",
    "\n",
    "以下技术帮助解决这些问题。\n",
    "\n",
    "## 4.1 权重初始化 (Weight Initialization)\n",
    "\n",
    "### 为什么初始化很重要？\n",
    "\n",
    "**错误示范 1：全零初始化**\n",
    "- 所有神经元输出相同\n",
    "- 梯度相同，更新相同\n",
    "- 网络无法打破对称性，无法学习\n",
    "\n",
    "**错误示范 2：随机初始化（方差太大）**\n",
    "- 激活值饱和（sigmoid 输出接近 0 或 1）\n",
    "- 梯度消失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比较不同初始化方法的权重分布\n",
    "diagrams.plot_init_distributions(['zero', 'random', 'xavier', 'he'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavier 初始化 (Glorot)\n",
    "\n",
    "适用于 tanh 激活函数：\n",
    "\n",
    "$$W \\sim \\mathcal{N}\\left(0, \\sqrt{\\frac{2}{n_{in} + n_{out}}}\\right)$$\n",
    "\n",
    "### He 初始化\n",
    "\n",
    "适用于 ReLU 激活函数：\n",
    "\n",
    "$$W \\sim \\mathcal{N}\\left(0, \\sqrt{\\frac{2}{n_{in}}}\\right)$$\n",
    "\n",
    "**核心思想**：保持每层激活值的方差稳定，防止梯度消失或爆炸。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.2 优化与批处理 (Optimization & Batching)\n",
    "\n",
    "### 批量大小的选择\n",
    "\n",
    "| 方式 | 批量大小 | 优点 | 缺点 |\n",
    "|------|----------|------|------|\n",
    "| 批量梯度下降 | 全部数据 | 稳定收敛 | 慢、内存大 |\n",
    "| 随机梯度下降 (SGD) | 1 | 快、正则化效果 | 噪声大、不稳定 |\n",
    "| 小批量 (Mini-batch) | 32-256 | 折中方案 | 需要调参 |\n",
    "\n",
    "### 优化器演进\n",
    "\n",
    "**1. SGD（随机梯度下降）**\n",
    "$$\\theta = \\theta - \\alpha \\nabla L$$\n",
    "- 简单直接，但在病态曲面上会震荡\n",
    "\n",
    "**2. Momentum（动量）**\n",
    "$$v_t = \\beta v_{t-1} + (1-\\beta) \\nabla L$$\n",
    "$$\\theta = \\theta - \\alpha v_t$$\n",
    "- 累积历史梯度方向，像球滚下山坡\n",
    "- 平滑震荡，加速收敛\n",
    "- $\\beta$ 通常取 0.9\n",
    "\n",
    "**3. RMSprop（均方根传播）**\n",
    "$$s_t = \\beta s_{t-1} + (1-\\beta) (\\nabla L)^2$$\n",
    "$$\\theta = \\theta - \\frac{\\alpha}{\\sqrt{s_t + \\epsilon}} \\nabla L$$\n",
    "- 跟踪梯度平方的移动平均\n",
    "- **自适应学习率**：梯度大的方向学习率小，梯度小的方向学习率大\n",
    "- 解决不同参数尺度不一致的问题\n",
    "\n",
    "**4. Adam（自适应矩估计）**\n",
    "$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) \\nabla L \\quad \\text{(一阶矩/动量)}$$\n",
    "$$v_t = \\beta_2 v_{t-1} + (1-\\beta_2) (\\nabla L)^2 \\quad \\text{(二阶矩/RMSprop)}$$\n",
    "$$\\theta = \\theta - \\frac{\\alpha}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t$$\n",
    "- 结合 Momentum + RMSprop\n",
    "- 默认 $\\beta_1=0.9$, $\\beta_2=0.999$\n",
    "- **现代深度学习的默认选择**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化动量效果：对比 SGD vs SGD+Momentum 的优化路径\n",
    "diagrams.plot_momentum_visualization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化学习率调度\n",
    "diagrams.plot_learning_rate_schedule('cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.3 归一化 (Normalization)\n",
    "\n",
    "### Batch Normalization\n",
    "\n",
    "在每一层激活之前，对小批量数据进行归一化：\n",
    "\n",
    "$$\\hat{x} = \\frac{x - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$$\n",
    "\n",
    "$$y = \\gamma \\hat{x} + \\beta$$\n",
    "\n",
    "其中 $\\gamma$ 和 $\\beta$ 是可学习参数。\n",
    "\n",
    "### 为什么有效？\n",
    "\n",
    "1. **减少内部协变量偏移**：每层输入分布更稳定\n",
    "2. **允许更大学习率**：加速训练\n",
    "3. **轻微正则化效果**：批量统计引入噪声"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalization\n",
    "\n",
    "对单个样本的所有特征归一化（而非批量方向）。\n",
    "\n",
    "**与 Batch Norm 的关键区别**：\n",
    "\n",
    "| 特性 | Batch Norm | Layer Norm |\n",
    "|------|------------|------------|\n",
    "| 归一化方向 | 跨样本（batch 维度） | 跨特征（layer 维度） |\n",
    "| 统计量计算 | 每个特征在 batch 内 | 每个样本在 layer 内 |\n",
    "| 依赖 batch size | 是 | 否 |\n",
    "| 推理时行为 | 使用训练时的移动平均 | 与训练时一致 |\n",
    "| 常用场景 | CNN、全连接网络 | RNN、Transformer |\n",
    "\n",
    "**为什么 Transformer 用 Layer Norm？**\n",
    "- 序列长度可变，batch 内样本数不固定\n",
    "- Layer Norm 对单个样本独立计算，不受 batch 影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化 Batch Norm vs Layer Norm 的归一化方向\n",
    "# 呼应 2.1 节的批处理维度图\n",
    "diagrams.plot_norm_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalization\n",
    "\n",
    "对单个样本的所有特征归一化（而非批量方向）。\n",
    "\n",
    "- 不依赖批量大小\n",
    "- 适用于 RNN、Transformer\n",
    "- 推理时行为与训练时一致"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.4 正则化与 Dropout\n",
    "\n",
    "### 过拟合问题\n",
    "\n",
    "当模型在训练集上表现很好，但在测试集上表现差时，就是**过拟合**。\n",
    "\n",
    "**症状**：训练损失持续下降，验证损失开始上升。\n",
    "\n",
    "**原因**：模型复杂度过高，\"记住\"了训练数据的噪声，而非学习真正的模式。\n",
    "\n",
    "### L2 正则化 (Weight Decay)\n",
    "\n",
    "在损失函数中添加权重惩罚项：\n",
    "\n",
    "$$L_{total} = L + \\frac{\\lambda}{2m} \\sum \\|W\\|^2$$\n",
    "\n",
    "**效果**：\n",
    "- 鼓励小权重，防止任何单一特征主导\n",
    "- 相当于对权重施加\"弹簧\"，拉向零点\n",
    "- $\\lambda$ 越大，正则化越强\n",
    "\n",
    "### Dropout\n",
    "\n",
    "训练时随机\"丢弃\"一部分神经元：\n",
    "\n",
    "| 阶段 | 行为 |\n",
    "|------|------|\n",
    "| 训练 | 每个神经元以概率 $p$ 被设为 0 |\n",
    "| 推理 | 使用全部神经元，权重乘以 $(1-p)$ |\n",
    "\n",
    "**为什么有效？**\n",
    "\n",
    "1. **防止共适应**：神经元不能依赖特定的其他神经元\n",
    "2. **隐式集成**：相当于训练 $2^n$ 个子网络的集成\n",
    "3. **类似噪声注入**：增加训练的鲁棒性\n",
    "\n",
    "**实践建议**：\n",
    "- 隐藏层常用 $p = 0.5$\n",
    "- 输入层用较小的 $p$（如 0.2）\n",
    "- 卷积层通常不用 Dropout（用 Batch Norm）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示过拟合曲线\n",
    "# 模拟训练过程\n",
    "epochs = 100\n",
    "train_loss = 2.0 * np.exp(-0.05 * np.arange(epochs)) + 0.1 * np.random.randn(epochs) * 0.1\n",
    "val_loss = 2.0 * np.exp(-0.03 * np.arange(epochs)) + 0.02 * np.arange(epochs) ** 0.5 + 0.1 * np.random.randn(epochs) * 0.1\n",
    "\n",
    "diagrams.plot_overfitting_curves(train_loss, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化 Dropout：训练时 vs 推理时\n",
    "diagrams.plot_dropout_network(drop_prob=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "\n",
    "监控验证集损失，当开始上升时停止训练。\n",
    "\n",
    "- 简单有效\n",
    "- 不需要额外超参数\n",
    "- 通常保存验证集上最佳的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 第五部分：实践\n",
    "\n",
    "## 5.1 MNIST 手写数字识别\n",
    "\n",
    "让我们用所学知识从零实现一个神经网络，识别手写数字。\n",
    "\n",
    "**任务**：输入 28×28 的灰度图像，输出 0-9 的数字类别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完整的神经网络实现\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \"\"\"简单的全连接神经网络\"\"\"\n",
    "    \n",
    "    def __init__(self, layer_dims):\n",
    "        \"\"\"\n",
    "        初始化网络\n",
    "        \n",
    "        参数:\n",
    "            layer_dims: 每层神经元数量，如 [784, 128, 64, 10]\n",
    "        \"\"\"\n",
    "        self.parameters = {}\n",
    "        self.L = len(layer_dims) - 1  # 层数（不含输入层）\n",
    "        \n",
    "        # He 初始化\n",
    "        for l in range(1, self.L + 1):\n",
    "            self.parameters[f'W{l}'] = np.random.randn(\n",
    "                layer_dims[l], layer_dims[l-1]\n",
    "            ) * np.sqrt(2.0 / layer_dims[l-1])\n",
    "            self.parameters[f'b{l}'] = np.zeros((layer_dims[l], 1))\n",
    "    \n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def relu_derivative(self, z):\n",
    "        return (z > 0).astype(float)\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"前向传播\"\"\"\n",
    "        self.cache = {'A0': X}\n",
    "        A = X\n",
    "        \n",
    "        # 隐藏层用 ReLU\n",
    "        for l in range(1, self.L):\n",
    "            Z = np.dot(self.parameters[f'W{l}'], A) + self.parameters[f'b{l}']\n",
    "            A = self.relu(Z)\n",
    "            self.cache[f'Z{l}'] = Z\n",
    "            self.cache[f'A{l}'] = A\n",
    "        \n",
    "        # 输出层用 Softmax\n",
    "        Z = np.dot(self.parameters[f'W{self.L}'], A) + self.parameters[f'b{self.L}']\n",
    "        A = self.softmax(Z)\n",
    "        self.cache[f'Z{self.L}'] = Z\n",
    "        self.cache[f'A{self.L}'] = A\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def compute_loss(self, Y):\n",
    "        \"\"\"交叉熵损失\"\"\"\n",
    "        m = Y.shape[1]\n",
    "        AL = self.cache[f'A{self.L}']\n",
    "        loss = -np.sum(Y * np.log(AL + 1e-8)) / m\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, Y):\n",
    "        \"\"\"反向传播\"\"\"\n",
    "        m = Y.shape[1]\n",
    "        self.grads = {}\n",
    "        \n",
    "        # 输出层\n",
    "        dZ = self.cache[f'A{self.L}'] - Y\n",
    "        self.grads[f'dW{self.L}'] = np.dot(dZ, self.cache[f'A{self.L-1}'].T) / m\n",
    "        self.grads[f'db{self.L}'] = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "        \n",
    "        # 隐藏层\n",
    "        for l in reversed(range(1, self.L)):\n",
    "            dA = np.dot(self.parameters[f'W{l+1}'].T, dZ)\n",
    "            dZ = dA * self.relu_derivative(self.cache[f'Z{l}'])\n",
    "            self.grads[f'dW{l}'] = np.dot(dZ, self.cache[f'A{l-1}'].T) / m\n",
    "            self.grads[f'db{l}'] = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    \n",
    "    def update(self, learning_rate):\n",
    "        \"\"\"更新参数\"\"\"\n",
    "        for l in range(1, self.L + 1):\n",
    "            self.parameters[f'W{l}'] -= learning_rate * self.grads[f'dW{l}']\n",
    "            self.parameters[f'b{l}'] -= learning_rate * self.grads[f'db{l}']\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"预测\"\"\"\n",
    "        A = self.forward(X)\n",
    "        return np.argmax(A, axis=0)\n",
    "\n",
    "print(\"神经网络类已定义！\")\n",
    "print(\"架构：输入 -> ReLU 隐藏层 -> Softmax 输出\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个小型网络进行演示\n",
    "# 实际训练需要加载 MNIST 数据集\n",
    "\n",
    "# 网络架构：784(输入) -> 128 -> 64 -> 10(输出)\n",
    "nn = NeuralNetwork([784, 128, 64, 10])\n",
    "\n",
    "print(\"网络参数形状：\")\n",
    "for key, value in nn.parameters.items():\n",
    "    print(f\"  {key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 MNIST 数据集\n",
    "import gzip\n",
    "import struct\n",
    "from pathlib import Path\n",
    "\n",
    "def load_mnist(data_dir='data/MNIST/raw'):\n",
    "    \"\"\"\n",
    "    从 IDX 格式加载 MNIST 数据集\n",
    "    \n",
    "    返回:\n",
    "        X_train: (784, 60000) 训练图像\n",
    "        y_train: (10, 60000) 训练标签 (one-hot)\n",
    "        X_test: (784, 10000) 测试图像\n",
    "        y_test: (10, 10000) 测试标签 (one-hot)\n",
    "    \"\"\"\n",
    "    data_path = Path(data_dir)\n",
    "    \n",
    "    def read_images(filename):\n",
    "        with gzip.open(data_path / filename, 'rb') as f:\n",
    "            magic, num, rows, cols = struct.unpack('>IIII', f.read(16))\n",
    "            images = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "            images = images.reshape(num, rows * cols).T  # (784, num)\n",
    "            return images / 255.0  # 归一化到 [0, 1]\n",
    "    \n",
    "    def read_labels(filename):\n",
    "        with gzip.open(data_path / filename, 'rb') as f:\n",
    "            magic, num = struct.unpack('>II', f.read(8))\n",
    "            labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "            # 转换为 one-hot 编码\n",
    "            one_hot = np.zeros((10, num))\n",
    "            one_hot[labels, np.arange(num)] = 1\n",
    "            return one_hot\n",
    "    \n",
    "    X_train = read_images('train-images-idx3-ubyte.gz')\n",
    "    y_train = read_labels('train-labels-idx1-ubyte.gz')\n",
    "    X_test = read_images('t10k-images-idx3-ubyte.gz')\n",
    "    y_test = read_labels('t10k-labels-idx1-ubyte.gz')\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# 加载数据\n",
    "X_train, y_train, X_test, y_test = load_mnist()\n",
    "\n",
    "print(f\"训练集: {X_train.shape[1]} 样本\")\n",
    "print(f\"测试集: {X_test.shape[1]} 样本\")\n",
    "print(f\"图像维度: {X_train.shape[0]} (28x28 展平)\")\n",
    "print(f\"类别数: {y_train.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化一些样本图像\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = X_train[:, i].reshape(28, 28)\n",
    "    label = np.argmax(y_train[:, i])\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(f'Label: {label}')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('MNIST Sample Images', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练神经网络\n",
    "def train(model, X_train, y_train, X_test, y_test, \n",
    "          epochs=10, batch_size=128, learning_rate=0.1):\n",
    "    \"\"\"\n",
    "    训练神经网络\n",
    "    \n",
    "    参数:\n",
    "        model: NeuralNetwork 实例\n",
    "        epochs: 训练轮数\n",
    "        batch_size: 小批量大小\n",
    "        learning_rate: 学习率\n",
    "    \"\"\"\n",
    "    m = X_train.shape[1]\n",
    "    history = {'train_loss': [], 'train_acc': [], 'test_acc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # 打乱数据\n",
    "        permutation = np.random.permutation(m)\n",
    "        X_shuffled = X_train[:, permutation]\n",
    "        y_shuffled = y_train[:, permutation]\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        num_batches = m // batch_size\n",
    "        \n",
    "        # Mini-batch 训练\n",
    "        for i in range(num_batches):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            X_batch = X_shuffled[:, start:end]\n",
    "            y_batch = y_shuffled[:, start:end]\n",
    "            \n",
    "            # 前向传播\n",
    "            model.forward(X_batch)\n",
    "            epoch_loss += model.compute_loss(y_batch)\n",
    "            \n",
    "            # 反向传播\n",
    "            model.backward(y_batch)\n",
    "            \n",
    "            # 更新参数\n",
    "            model.update(learning_rate)\n",
    "        \n",
    "        # 计算指标\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        train_pred = model.predict(X_train)\n",
    "        train_acc = np.mean(train_pred == np.argmax(y_train, axis=0))\n",
    "        test_pred = model.predict(X_test)\n",
    "        test_acc = np.mean(test_pred == np.argmax(y_test, axis=0))\n",
    "        \n",
    "        history['train_loss'].append(avg_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:2d}/{epochs} | Loss: {avg_loss:.4f} | Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "print(\"训练函数已定义！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建并训练网络\n",
    "# 架构：784 -> 128 -> 64 -> 10\n",
    "np.random.seed(42)\n",
    "model = NeuralNetwork([784, 64, 64, 10])\n",
    "\n",
    "print(\"开始训练...\")\n",
    "print(\"-\" * 60)\n",
    "history = train(model, X_train, y_train, X_test, y_test, \n",
    "                epochs=10, batch_size=128, learning_rate=0.1)\n",
    "print(\"-\" * 60)\n",
    "print(\"训练完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化训练过程\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# 损失曲线\n",
    "axes[0].plot(history['train_loss'], 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 准确率曲线\n",
    "axes[1].plot(history['train_acc'], 'b-', linewidth=2, label='Train')\n",
    "axes[1].plot(history['test_acc'], 'r-', linewidth=2, label='Test')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].set_title('Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n最终测试准确率: {history['test_acc'][-1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化一些预测结果\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "\n",
    "# 随机选择测试样本\n",
    "indices = np.random.choice(X_test.shape[1], 10, replace=False)\n",
    "\n",
    "for i, (ax, idx) in enumerate(zip(axes.flat, indices)):\n",
    "    img = X_test[:, idx].reshape(28, 28)\n",
    "    true_label = np.argmax(y_test[:, idx])\n",
    "    pred_label = model.predict(X_test[:, idx:idx+1])[0]\n",
    "    \n",
    "    ax.imshow(img, cmap='gray')\n",
    "    color = 'green' if pred_label == true_label else 'red'\n",
    "    ax.set_title(f'Pred: {pred_label} (True: {true_label})', color=color)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Model Predictions on Test Set', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化第一层学到的特征（权重）\n",
    "fig, axes = plt.subplots(4, 8, figsize=(12, 6))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < model.parameters['W1'].shape[0]:\n",
    "        # 第一层每个神经元的权重可以reshape成28x28的图像\n",
    "        weights = model.parameters['W1'][i].reshape(28, 28)\n",
    "        ax.imshow(weights, cmap='RdBu', vmin=-0.5, vmax=0.5)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('First Layer Learned Features (Weights)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 练习题\n",
    "\n",
    "### 练习 1：手动计算前向传播\n",
    "\n",
    "给定一个 2 层网络：\n",
    "- 输入：$x = [1, 2]^T$\n",
    "- $W^{[1]} = \\begin{bmatrix} 0.1 & 0.2 \\\\ 0.3 & 0.4 \\end{bmatrix}$，$b^{[1]} = [0, 0]^T$\n",
    "- $W^{[2]} = [0.5, 0.6]$，$b^{[2]} = 0$\n",
    "- 激活函数：sigmoid\n",
    "\n",
    "计算输出 $\\hat{y}$。\n",
    "\n",
    "### 练习 2：实现 Dropout\n",
    "\n",
    "修改 `NeuralNetwork` 类，在训练时添加 Dropout。\n",
    "\n",
    "### 练习 3：比较优化器\n",
    "\n",
    "实现 SGD with Momentum 和 Adam，在 MNIST 上比较收敛速度。\n",
    "\n",
    "### 练习 4：可视化特征\n",
    "\n",
    "训练网络后，可视化第一层权重，看看网络学到了什么特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 总结\n",
    "\n",
    "## 关键概念回顾\n",
    "\n",
    "| 概念 | 要点 |\n",
    "|------|------|\n",
    "| 感知机 | 线性分类器，决策边界是超平面 |\n",
    "| 神经元 | 感知机 + 可微分激活函数 |\n",
    "| 激活函数 | 引入非线性，ReLU 最常用 |\n",
    "| 前向传播 | 从输入到输出的计算流程 |\n",
    "| 损失函数 | 衡量预测误差，分类用交叉熵 |\n",
    "| 反向传播 | 链式法则计算梯度 |\n",
    "| 深度的意义 | 特征层次、参数效率 |\n",
    "| 初始化 | Xavier/He，防止梯度消失 |\n",
    "| Batch Norm | 稳定训练，允许更大学习率 |\n",
    "| Dropout | 正则化，防止过拟合 |\n",
    "\n",
    "## 下一步学习\n",
    "\n",
    "- 卷积神经网络 (CNN)\n",
    "- 循环神经网络 (RNN)\n",
    "- Transformer 架构\n",
    "- 现代优化技巧（学习率调度、梯度裁剪）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 参考资料\n",
    "\n",
    "- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) - Michael Nielsen\n",
    "- [Deep Learning Book](https://www.deeplearningbook.org/) - Goodfellow, Bengio, Courville\n",
    "- [CS231n: Convolutional Neural Networks](http://cs231n.stanford.edu/) - Stanford\n",
    "- [3Blue1Brown: Neural Networks](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) - YouTube\n",
    "- [Welch Labs: Neural Networks](https://www.youtube.com/watch?v=NrO20Jb-hy0) - Youtube"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
